{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zkweng/venv_ros/install/venv_ros/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/zkweng/.cache/kagglehub/datasets/bestwater/wikitext-2-v1/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bestwater/wikitext-2-v1\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THEN MOVE THE FILE DIRECTORY TO THIS DIRECTORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText2 corpus paths\n",
    "corpus_path = './wikitext-2'\n",
    "train_path = os.path.join(corpus_path, 'wiki.train.tokens')\n",
    "valid_path = os.path.join(corpus_path, 'wiki.valid.tokens')\n",
    "test_path = os.path.join(corpus_path, 'wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "dropout_prob = 0.5\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "seq_length = 20  # Number of unrolled time steps\n",
    "learning_rate = 0.001\n",
    "vocab_size = 10000  # Reduced vocabulary size\n",
    "unk_threshold = 5  # Frequency threshold for unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    \"\"\"Read corpus file and return list of whitespace-tokenized words\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().replace('\\n', ' <eos> ')\n",
    "    return text.split(' ')\n",
    "\n",
    "def build_vocab(tokens, threshold=unk_threshold):\n",
    "    \"\"\"Build vocabulary from tokens with frequency threshold\"\"\"\n",
    "    counter = Counter(tokens)\n",
    "    # Sort tokens by frequency (descending)\n",
    "    sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create vocabulary: reserve 0 for padding, 1 for <unk>\n",
    "    vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2}\n",
    "    idx = 3\n",
    "    \n",
    "    # Add tokens that appear more than threshold times\n",
    "    for token, count in sorted_tokens:\n",
    "        if count >= threshold and idx < vocab_size:\n",
    "            if token and token != '<eos>':  # Skip empty tokens and already added special tokens\n",
    "                vocab[token] = idx\n",
    "                idx += 1\n",
    "        if idx >= vocab_size:\n",
    "            break\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def tokens_to_indices(tokens, vocab):\n",
    "    \"\"\"Convert tokens to indices using vocabulary\"\"\"\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens if token]\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \"\"\"Divide dataset into batches and arrange for back-propagation through time\"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data[:nbatch * bsz]\n",
    "    # Evenly divide the data across the bsz batches\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i, seq_length):\n",
    "    \"\"\"Get a batch for training\"\"\"\n",
    "    seq_len = min(seq_length, source.size(0) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=False, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights for better training\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # x shape: (seq_len, batch_size)\n",
    "        emb = self.dropout(self.embedding(x))  # (seq_len, batch_size, embed_dim)\n",
    "        output, hidden = self.rnn(emb, hidden)  # output: (seq_len, batch_size, hidden_dim)\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.decoder(output.view(-1, self.hidden_dim))  # (seq_len*batch_size, vocab_size)\n",
    "        return decoded, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, criterion, optimizer, seq_length):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get total number of batches\n",
    "    num_batches = (train_data.size(0) - 1) // seq_length\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n",
    "        # Skip if we'd go out of bounds\n",
    "        if i > train_data.size(0) - 2:\n",
    "            continue\n",
    "            \n",
    "        data, targets = get_batch(train_data, i, seq_length)\n",
    "        \n",
    "        # Initialize hidden state for new batch\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch % 50 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 50\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch {epoch:3d} | batch {batch:5d}/{num_batches:5d} | '\n",
    "                  f'ms/batch {elapsed * 1000 / 50:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "def evaluate(model, eval_data, criterion, seq_length):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    num_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, seq_length):\n",
    "            # Skip if we'd go out of bounds\n",
    "            if i > eval_data.size(0) - 2:\n",
    "                continue\n",
    "                \n",
    "            data, targets = get_batch(eval_data, i, seq_length)\n",
    "            hidden = hidden.detach()\n",
    "            output, hidden = model(data, hidden)\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            num_tokens += targets.size(0)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / num_tokens if num_tokens > 0 else float('inf')\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing corpus...\n",
      "Vocabulary size: 9999\n",
      "Warning: Found index 9999 but vocab size is 9999\n",
      "Fixing indices...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zkweng/venv_ros/install/venv_ros/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing corpus...\")\n",
    "    train_tokens = read_corpus(train_path)\n",
    "    valid_tokens = read_corpus(valid_path)\n",
    "    test_tokens = read_corpus(test_path)\n",
    "\n",
    "    # Build vocabulary from training tokens\n",
    "    vocab = build_vocab(train_tokens)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    train_indices = tokens_to_indices(train_tokens, vocab)\n",
    "    valid_indices = tokens_to_indices(valid_tokens, vocab)\n",
    "    test_indices = tokens_to_indices(test_tokens, vocab)\n",
    "\n",
    "    # Safety check: ensure all indices are valid\n",
    "    max_idx = max(train_indices + valid_indices + test_indices)\n",
    "    if max_idx >= len(vocab):\n",
    "        print(f\"Warning: Found index {max_idx} but vocab size is {len(vocab)}\")\n",
    "        print(\"Fixing indices...\")\n",
    "        train_indices = [min(idx, len(vocab) - 1) for idx in train_indices]\n",
    "        valid_indices = [min(idx, len(vocab) - 1) for idx in valid_indices]\n",
    "        test_indices = [min(idx, len(vocab) - 1) for idx in test_indices]\n",
    "\n",
    "    # Convert to tensors and batchify - use CPU first for safety\n",
    "    train_data = batchify(torch.tensor(train_indices, dtype=torch.long), batch_size)\n",
    "    valid_data = batchify(torch.tensor(valid_indices, dtype=torch.long), batch_size)\n",
    "    test_data = batchify(torch.tensor(test_indices, dtype=torch.long), batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = RNNModel(len(vocab), embed_dim, hidden_dim, dropout_prob).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| epoch   1 | batch    50/ 3263 | ms/batch  7.87 | loss  7.19 | ppl  1331.74\n",
      "| epoch   1 | batch   100/ 3263 | ms/batch  4.24 | loss  6.38 | ppl   592.34\n",
      "| epoch   1 | batch   150/ 3263 | ms/batch  4.20 | loss  6.19 | ppl   486.03\n",
      "| epoch   1 | batch   200/ 3263 | ms/batch  4.12 | loss  6.08 | ppl   437.33\n",
      "| epoch   1 | batch   250/ 3263 | ms/batch  4.24 | loss  5.96 | ppl   388.67\n",
      "| epoch   1 | batch   300/ 3263 | ms/batch  4.22 | loss  5.88 | ppl   359.57\n",
      "| epoch   1 | batch   350/ 3263 | ms/batch  4.31 | loss  5.88 | ppl   357.22\n",
      "| epoch   1 | batch   400/ 3263 | ms/batch  4.30 | loss  5.88 | ppl   356.52\n",
      "| epoch   1 | batch   450/ 3263 | ms/batch  4.25 | loss  5.86 | ppl   351.13\n",
      "| epoch   1 | batch   500/ 3263 | ms/batch  4.15 | loss  5.78 | ppl   323.83\n",
      "| epoch   1 | batch   550/ 3263 | ms/batch  4.25 | loss  5.80 | ppl   329.24\n",
      "| epoch   1 | batch   600/ 3263 | ms/batch  4.30 | loss  5.70 | ppl   299.36\n",
      "| epoch   1 | batch   650/ 3263 | ms/batch  4.20 | loss  5.66 | ppl   288.48\n",
      "| epoch   1 | batch   700/ 3263 | ms/batch  4.45 | loss  5.65 | ppl   283.76\n",
      "| epoch   1 | batch   750/ 3263 | ms/batch  4.20 | loss  5.61 | ppl   272.68\n",
      "| epoch   1 | batch   800/ 3263 | ms/batch  4.20 | loss  5.55 | ppl   257.41\n",
      "| epoch   1 | batch   850/ 3263 | ms/batch  4.24 | loss  5.52 | ppl   250.81\n",
      "| epoch   1 | batch   900/ 3263 | ms/batch  4.58 | loss  5.57 | ppl   263.23\n",
      "| epoch   1 | batch   950/ 3263 | ms/batch  4.53 | loss  5.54 | ppl   255.62\n",
      "| epoch   1 | batch  1000/ 3263 | ms/batch  4.19 | loss  5.58 | ppl   265.94\n",
      "| epoch   1 | batch  1050/ 3263 | ms/batch  4.16 | loss  5.55 | ppl   257.82\n",
      "| epoch   1 | batch  1100/ 3263 | ms/batch  4.13 | loss  5.57 | ppl   261.25\n",
      "| epoch   1 | batch  1150/ 3263 | ms/batch  4.15 | loss  5.56 | ppl   259.42\n",
      "| epoch   1 | batch  1200/ 3263 | ms/batch  4.10 | loss  5.49 | ppl   243.28\n",
      "| epoch   1 | batch  1250/ 3263 | ms/batch  4.11 | loss  5.46 | ppl   235.30\n",
      "| epoch   1 | batch  1300/ 3263 | ms/batch  4.42 | loss  5.49 | ppl   241.93\n",
      "| epoch   1 | batch  1350/ 3263 | ms/batch  4.30 | loss  5.50 | ppl   243.85\n",
      "| epoch   1 | batch  1400/ 3263 | ms/batch  4.20 | loss  5.52 | ppl   250.69\n",
      "| epoch   1 | batch  1450/ 3263 | ms/batch  4.19 | loss  5.47 | ppl   237.57\n",
      "| epoch   1 | batch  1500/ 3263 | ms/batch  4.12 | loss  5.50 | ppl   244.22\n",
      "| epoch   1 | batch  1550/ 3263 | ms/batch  4.27 | loss  5.49 | ppl   242.51\n",
      "| epoch   1 | batch  1600/ 3263 | ms/batch  4.47 | loss  5.51 | ppl   246.31\n",
      "| epoch   1 | batch  1650/ 3263 | ms/batch  4.34 | loss  5.47 | ppl   237.76\n",
      "| epoch   1 | batch  1700/ 3263 | ms/batch  4.06 | loss  5.42 | ppl   225.87\n",
      "| epoch   1 | batch  1750/ 3263 | ms/batch  3.94 | loss  5.39 | ppl   218.63\n",
      "| epoch   1 | batch  1800/ 3263 | ms/batch  3.96 | loss  5.44 | ppl   230.89\n",
      "| epoch   1 | batch  1850/ 3263 | ms/batch  3.97 | loss  5.42 | ppl   225.41\n",
      "| epoch   1 | batch  1900/ 3263 | ms/batch  3.98 | loss  5.37 | ppl   215.35\n",
      "| epoch   1 | batch  1950/ 3263 | ms/batch  3.94 | loss  5.38 | ppl   217.72\n",
      "| epoch   1 | batch  2000/ 3263 | ms/batch  4.14 | loss  5.41 | ppl   224.31\n",
      "| epoch   1 | batch  2050/ 3263 | ms/batch  4.14 | loss  5.44 | ppl   231.08\n",
      "| epoch   1 | batch  2100/ 3263 | ms/batch  4.23 | loss  5.40 | ppl   221.77\n",
      "| epoch   1 | batch  2150/ 3263 | ms/batch  4.14 | loss  5.42 | ppl   224.80\n",
      "| epoch   1 | batch  2200/ 3263 | ms/batch  4.36 | loss  5.40 | ppl   221.42\n",
      "| epoch   1 | batch  2250/ 3263 | ms/batch  4.17 | loss  5.35 | ppl   210.55\n",
      "| epoch   1 | batch  2300/ 3263 | ms/batch  4.14 | loss  5.40 | ppl   222.41\n",
      "| epoch   1 | batch  2350/ 3263 | ms/batch  4.20 | loss  5.35 | ppl   210.97\n",
      "| epoch   1 | batch  2400/ 3263 | ms/batch  4.15 | loss  5.34 | ppl   208.70\n",
      "| epoch   1 | batch  2450/ 3263 | ms/batch  4.22 | loss  5.37 | ppl   214.54\n",
      "| epoch   1 | batch  2500/ 3263 | ms/batch  4.21 | loss  5.35 | ppl   211.66\n",
      "| epoch   1 | batch  2550/ 3263 | ms/batch  4.18 | loss  5.31 | ppl   201.63\n",
      "| epoch   1 | batch  2600/ 3263 | ms/batch  4.11 | loss  5.32 | ppl   203.74\n",
      "| epoch   1 | batch  2650/ 3263 | ms/batch  4.12 | loss  5.30 | ppl   201.02\n",
      "| epoch   1 | batch  2700/ 3263 | ms/batch  4.15 | loss  5.36 | ppl   212.67\n",
      "| epoch   1 | batch  2750/ 3263 | ms/batch  4.56 | loss  5.38 | ppl   216.44\n",
      "| epoch   1 | batch  2800/ 3263 | ms/batch  4.10 | loss  5.39 | ppl   219.12\n",
      "| epoch   1 | batch  2850/ 3263 | ms/batch  4.20 | loss  5.35 | ppl   210.70\n",
      "| epoch   1 | batch  2900/ 3263 | ms/batch  4.10 | loss  5.32 | ppl   203.68\n",
      "| epoch   1 | batch  2950/ 3263 | ms/batch  4.16 | loss  5.37 | ppl   214.52\n",
      "| epoch   1 | batch  3000/ 3263 | ms/batch  4.22 | loss  5.31 | ppl   202.50\n",
      "| epoch   1 | batch  3050/ 3263 | ms/batch  4.11 | loss  5.29 | ppl   198.15\n",
      "| epoch   1 | batch  3100/ 3263 | ms/batch  4.13 | loss  5.28 | ppl   196.13\n",
      "| epoch   1 | batch  3150/ 3263 | ms/batch  4.72 | loss  5.28 | ppl   197.11\n",
      "| epoch   1 | batch  3200/ 3263 | ms/batch  4.65 | loss  5.28 | ppl   195.40\n",
      "| epoch   1 | batch  3250/ 3263 | ms/batch  4.12 | loss  5.20 | ppl   181.60\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 14.25s | valid ppl   152.70\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   152.70\n",
      "| epoch   2 | batch    50/ 3263 | ms/batch  4.22 | loss  5.30 | ppl   200.48\n",
      "| epoch   2 | batch   100/ 3263 | ms/batch  4.14 | loss  5.22 | ppl   185.68\n",
      "| epoch   2 | batch   150/ 3263 | ms/batch  4.35 | loss  5.17 | ppl   175.17\n",
      "| epoch   2 | batch   200/ 3263 | ms/batch  3.91 | loss  5.18 | ppl   177.50\n",
      "| epoch   2 | batch   250/ 3263 | ms/batch  3.94 | loss  5.17 | ppl   176.59\n",
      "| epoch   2 | batch   300/ 3263 | ms/batch  3.96 | loss  5.15 | ppl   172.55\n",
      "| epoch   2 | batch   350/ 3263 | ms/batch  3.94 | loss  5.18 | ppl   177.21\n",
      "| epoch   2 | batch   400/ 3263 | ms/batch  3.88 | loss  5.17 | ppl   176.33\n",
      "| epoch   2 | batch   450/ 3263 | ms/batch  3.88 | loss  5.23 | ppl   186.03\n",
      "| epoch   2 | batch   500/ 3263 | ms/batch  3.91 | loss  5.17 | ppl   175.65\n",
      "| epoch   2 | batch   550/ 3263 | ms/batch  3.90 | loss  5.20 | ppl   181.38\n",
      "| epoch   2 | batch   600/ 3263 | ms/batch  4.07 | loss  5.15 | ppl   172.76\n",
      "| epoch   2 | batch   650/ 3263 | ms/batch  3.98 | loss  5.14 | ppl   170.66\n",
      "| epoch   2 | batch   700/ 3263 | ms/batch  4.05 | loss  5.14 | ppl   171.00\n",
      "| epoch   2 | batch   750/ 3263 | ms/batch  4.08 | loss  5.12 | ppl   167.95\n",
      "| epoch   2 | batch   800/ 3263 | ms/batch  4.14 | loss  5.11 | ppl   165.92\n",
      "| epoch   2 | batch   850/ 3263 | ms/batch  4.09 | loss  5.09 | ppl   162.78\n",
      "| epoch   2 | batch   900/ 3263 | ms/batch  4.21 | loss  5.16 | ppl   173.36\n",
      "| epoch   2 | batch   950/ 3263 | ms/batch  3.88 | loss  5.14 | ppl   170.13\n",
      "| epoch   2 | batch  1000/ 3263 | ms/batch  3.91 | loss  5.18 | ppl   176.88\n",
      "| epoch   2 | batch  1050/ 3263 | ms/batch  3.90 | loss  5.09 | ppl   162.95\n",
      "| epoch   2 | batch  1100/ 3263 | ms/batch  3.91 | loss  5.16 | ppl   173.36\n",
      "| epoch   2 | batch  1150/ 3263 | ms/batch  3.91 | loss  5.16 | ppl   174.69\n",
      "| epoch   2 | batch  1200/ 3263 | ms/batch  3.90 | loss  5.14 | ppl   170.05\n",
      "| epoch   2 | batch  1250/ 3263 | ms/batch  4.34 | loss  5.11 | ppl   166.08\n",
      "| epoch   2 | batch  1300/ 3263 | ms/batch  4.06 | loss  5.14 | ppl   170.27\n",
      "| epoch   2 | batch  1350/ 3263 | ms/batch  4.09 | loss  5.14 | ppl   171.55\n",
      "| epoch   2 | batch  1400/ 3263 | ms/batch  4.18 | loss  5.16 | ppl   174.93\n",
      "| epoch   2 | batch  1450/ 3263 | ms/batch  4.08 | loss  5.12 | ppl   167.18\n",
      "| epoch   2 | batch  1500/ 3263 | ms/batch  4.23 | loss  5.15 | ppl   172.35\n",
      "| epoch   2 | batch  1550/ 3263 | ms/batch  4.21 | loss  5.15 | ppl   172.23\n",
      "| epoch   2 | batch  1600/ 3263 | ms/batch  4.06 | loss  5.19 | ppl   179.34\n",
      "| epoch   2 | batch  1650/ 3263 | ms/batch  4.08 | loss  5.15 | ppl   172.17\n",
      "| epoch   2 | batch  1700/ 3263 | ms/batch  4.12 | loss  5.10 | ppl   163.59\n",
      "| epoch   2 | batch  1750/ 3263 | ms/batch  4.06 | loss  5.10 | ppl   163.30\n",
      "| epoch   2 | batch  1800/ 3263 | ms/batch  4.33 | loss  5.13 | ppl   168.75\n",
      "| epoch   2 | batch  1850/ 3263 | ms/batch  4.08 | loss  5.13 | ppl   168.41\n",
      "| epoch   2 | batch  1900/ 3263 | ms/batch  4.10 | loss  5.08 | ppl   161.39\n",
      "| epoch   2 | batch  1950/ 3263 | ms/batch  4.08 | loss  5.12 | ppl   166.59\n",
      "| epoch   2 | batch  2000/ 3263 | ms/batch  4.14 | loss  5.14 | ppl   170.13\n",
      "| epoch   2 | batch  2050/ 3263 | ms/batch  4.05 | loss  5.18 | ppl   176.91\n",
      "| epoch   2 | batch  2100/ 3263 | ms/batch  4.13 | loss  5.13 | ppl   169.84\n",
      "| epoch   2 | batch  2150/ 3263 | ms/batch  4.17 | loss  5.15 | ppl   171.89\n",
      "| epoch   2 | batch  2200/ 3263 | ms/batch  4.08 | loss  5.13 | ppl   168.53\n",
      "| epoch   2 | batch  2250/ 3263 | ms/batch  4.40 | loss  5.06 | ppl   158.22\n",
      "| epoch   2 | batch  2300/ 3263 | ms/batch  4.21 | loss  5.10 | ppl   164.30\n",
      "| epoch   2 | batch  2350/ 3263 | ms/batch  4.17 | loss  5.06 | ppl   157.73\n",
      "| epoch   2 | batch  2400/ 3263 | ms/batch  4.05 | loss  5.06 | ppl   157.16\n",
      "| epoch   2 | batch  2450/ 3263 | ms/batch  4.15 | loss  5.07 | ppl   159.35\n",
      "| epoch   2 | batch  2500/ 3263 | ms/batch  4.06 | loss  5.08 | ppl   161.45\n",
      "| epoch   2 | batch  2550/ 3263 | ms/batch  4.17 | loss  5.05 | ppl   155.74\n",
      "| epoch   2 | batch  2600/ 3263 | ms/batch  4.07 | loss  5.07 | ppl   158.88\n",
      "| epoch   2 | batch  2650/ 3263 | ms/batch  3.93 | loss  5.04 | ppl   154.45\n",
      "| epoch   2 | batch  2700/ 3263 | ms/batch  3.90 | loss  5.09 | ppl   163.05\n",
      "| epoch   2 | batch  2750/ 3263 | ms/batch  3.89 | loss  5.12 | ppl   167.22\n",
      "| epoch   2 | batch  2800/ 3263 | ms/batch  3.87 | loss  5.16 | ppl   173.87\n",
      "| epoch   2 | batch  2850/ 3263 | ms/batch  3.87 | loss  5.10 | ppl   163.84\n",
      "| epoch   2 | batch  2900/ 3263 | ms/batch  3.89 | loss  5.09 | ppl   161.70\n",
      "| epoch   2 | batch  2950/ 3263 | ms/batch  3.87 | loss  5.12 | ppl   167.09\n",
      "| epoch   2 | batch  3000/ 3263 | ms/batch  3.90 | loss  5.09 | ppl   162.15\n",
      "| epoch   2 | batch  3050/ 3263 | ms/batch  3.86 | loss  5.07 | ppl   158.54\n",
      "| epoch   2 | batch  3100/ 3263 | ms/batch  3.91 | loss  5.06 | ppl   156.87\n",
      "| epoch   2 | batch  3150/ 3263 | ms/batch  3.90 | loss  5.08 | ppl   161.25\n",
      "| epoch   2 | batch  3200/ 3263 | ms/batch  3.91 | loss  5.08 | ppl   161.36\n",
      "| epoch   2 | batch  3250/ 3263 | ms/batch  3.88 | loss  5.03 | ppl   152.47\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 13.49s | valid ppl   132.74\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   132.74\n",
      "| epoch   3 | batch    50/ 3263 | ms/batch  3.97 | loss  5.13 | ppl   168.44\n",
      "| epoch   3 | batch   100/ 3263 | ms/batch  3.88 | loss  5.03 | ppl   152.31\n",
      "| epoch   3 | batch   150/ 3263 | ms/batch  3.88 | loss  4.98 | ppl   145.81\n",
      "| epoch   3 | batch   200/ 3263 | ms/batch  3.87 | loss  5.00 | ppl   148.04\n",
      "| epoch   3 | batch   250/ 3263 | ms/batch  3.92 | loss  5.00 | ppl   148.94\n",
      "| epoch   3 | batch   300/ 3263 | ms/batch  3.91 | loss  4.96 | ppl   142.96\n",
      "| epoch   3 | batch   350/ 3263 | ms/batch  3.95 | loss  5.01 | ppl   149.61\n",
      "| epoch   3 | batch   400/ 3263 | ms/batch  3.89 | loss  4.99 | ppl   147.66\n",
      "| epoch   3 | batch   450/ 3263 | ms/batch  3.89 | loss  5.05 | ppl   155.96\n",
      "| epoch   3 | batch   500/ 3263 | ms/batch  3.90 | loss  4.99 | ppl   146.31\n",
      "| epoch   3 | batch   550/ 3263 | ms/batch  3.91 | loss  5.02 | ppl   150.97\n",
      "| epoch   3 | batch   600/ 3263 | ms/batch  3.88 | loss  4.99 | ppl   147.07\n",
      "| epoch   3 | batch   650/ 3263 | ms/batch  3.90 | loss  4.98 | ppl   145.36\n",
      "| epoch   3 | batch   700/ 3263 | ms/batch  3.90 | loss  4.99 | ppl   147.17\n",
      "| epoch   3 | batch   750/ 3263 | ms/batch  3.93 | loss  4.98 | ppl   145.04\n",
      "| epoch   3 | batch   800/ 3263 | ms/batch  3.90 | loss  4.98 | ppl   145.00\n",
      "| epoch   3 | batch   850/ 3263 | ms/batch  3.90 | loss  4.95 | ppl   141.70\n",
      "| epoch   3 | batch   900/ 3263 | ms/batch  3.89 | loss  5.02 | ppl   150.83\n",
      "| epoch   3 | batch   950/ 3263 | ms/batch  3.88 | loss  4.99 | ppl   146.74\n",
      "| epoch   3 | batch  1000/ 3263 | ms/batch  3.89 | loss  5.02 | ppl   151.73\n",
      "| epoch   3 | batch  1050/ 3263 | ms/batch  3.91 | loss  4.93 | ppl   138.66\n",
      "| epoch   3 | batch  1100/ 3263 | ms/batch  3.89 | loss  5.02 | ppl   151.56\n",
      "| epoch   3 | batch  1150/ 3263 | ms/batch  3.88 | loss  5.02 | ppl   151.31\n",
      "| epoch   3 | batch  1200/ 3263 | ms/batch  3.88 | loss  5.00 | ppl   148.03\n",
      "| epoch   3 | batch  1250/ 3263 | ms/batch  3.90 | loss  4.98 | ppl   145.27\n",
      "| epoch   3 | batch  1300/ 3263 | ms/batch  3.90 | loss  5.01 | ppl   149.24\n",
      "| epoch   3 | batch  1350/ 3263 | ms/batch  3.91 | loss  5.01 | ppl   149.70\n",
      "| epoch   3 | batch  1400/ 3263 | ms/batch  3.88 | loss  5.05 | ppl   155.49\n",
      "| epoch   3 | batch  1450/ 3263 | ms/batch  3.90 | loss  4.98 | ppl   145.99\n",
      "| epoch   3 | batch  1500/ 3263 | ms/batch  3.90 | loss  5.01 | ppl   149.78\n",
      "| epoch   3 | batch  1550/ 3263 | ms/batch  3.90 | loss  5.01 | ppl   150.63\n",
      "| epoch   3 | batch  1600/ 3263 | ms/batch  3.88 | loss  5.06 | ppl   156.89\n",
      "| epoch   3 | batch  1650/ 3263 | ms/batch  3.90 | loss  5.02 | ppl   151.70\n",
      "| epoch   3 | batch  1700/ 3263 | ms/batch  3.90 | loss  4.97 | ppl   143.44\n",
      "| epoch   3 | batch  1750/ 3263 | ms/batch  3.90 | loss  4.96 | ppl   142.60\n",
      "| epoch   3 | batch  1800/ 3263 | ms/batch  3.91 | loss  4.98 | ppl   145.11\n",
      "| epoch   3 | batch  1850/ 3263 | ms/batch  3.91 | loss  5.01 | ppl   150.59\n",
      "| epoch   3 | batch  1900/ 3263 | ms/batch  3.91 | loss  4.96 | ppl   142.49\n",
      "| epoch   3 | batch  1950/ 3263 | ms/batch  3.89 | loss  5.00 | ppl   147.84\n",
      "| epoch   3 | batch  2000/ 3263 | ms/batch  3.90 | loss  5.03 | ppl   152.25\n",
      "| epoch   3 | batch  2050/ 3263 | ms/batch  3.91 | loss  5.07 | ppl   158.94\n",
      "| epoch   3 | batch  2100/ 3263 | ms/batch  3.90 | loss  5.03 | ppl   153.20\n",
      "| epoch   3 | batch  2150/ 3263 | ms/batch  3.89 | loss  5.02 | ppl   152.07\n",
      "| epoch   3 | batch  2200/ 3263 | ms/batch  3.90 | loss  5.01 | ppl   149.34\n",
      "| epoch   3 | batch  2250/ 3263 | ms/batch  3.90 | loss  4.93 | ppl   138.00\n",
      "| epoch   3 | batch  2300/ 3263 | ms/batch  3.88 | loss  4.99 | ppl   146.37\n",
      "| epoch   3 | batch  2350/ 3263 | ms/batch  3.95 | loss  4.92 | ppl   137.53\n",
      "| epoch   3 | batch  2400/ 3263 | ms/batch  3.90 | loss  4.95 | ppl   140.48\n",
      "| epoch   3 | batch  2450/ 3263 | ms/batch  3.91 | loss  4.94 | ppl   140.43\n",
      "| epoch   3 | batch  2500/ 3263 | ms/batch  3.90 | loss  4.96 | ppl   142.35\n",
      "| epoch   3 | batch  2550/ 3263 | ms/batch  3.90 | loss  4.93 | ppl   137.78\n",
      "| epoch   3 | batch  2600/ 3263 | ms/batch  3.89 | loss  4.96 | ppl   142.49\n",
      "| epoch   3 | batch  2650/ 3263 | ms/batch  3.91 | loss  4.94 | ppl   139.19\n",
      "| epoch   3 | batch  2700/ 3263 | ms/batch  3.89 | loss  5.00 | ppl   148.10\n",
      "| epoch   3 | batch  2750/ 3263 | ms/batch  3.91 | loss  5.00 | ppl   147.67\n",
      "| epoch   3 | batch  2800/ 3263 | ms/batch  3.89 | loss  5.04 | ppl   154.85\n",
      "| epoch   3 | batch  2850/ 3263 | ms/batch  3.88 | loss  4.98 | ppl   145.63\n",
      "| epoch   3 | batch  2900/ 3263 | ms/batch  3.90 | loss  4.97 | ppl   144.51\n",
      "| epoch   3 | batch  2950/ 3263 | ms/batch  3.90 | loss  5.01 | ppl   149.57\n",
      "| epoch   3 | batch  3000/ 3263 | ms/batch  3.90 | loss  4.96 | ppl   143.24\n",
      "| epoch   3 | batch  3050/ 3263 | ms/batch  3.89 | loss  4.96 | ppl   142.28\n",
      "| epoch   3 | batch  3100/ 3263 | ms/batch  3.88 | loss  4.95 | ppl   140.78\n",
      "| epoch   3 | batch  3150/ 3263 | ms/batch  3.89 | loss  4.98 | ppl   145.21\n",
      "| epoch   3 | batch  3200/ 3263 | ms/batch  3.89 | loss  4.99 | ppl   146.34\n",
      "| epoch   3 | batch  3250/ 3263 | ms/batch  4.02 | loss  4.94 | ppl   140.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 13.04s | valid ppl   123.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   123.76\n",
      "| epoch   4 | batch    50/ 3263 | ms/batch  4.23 | loss  5.03 | ppl   153.57\n",
      "| epoch   4 | batch   100/ 3263 | ms/batch  4.14 | loss  4.92 | ppl   137.49\n",
      "| epoch   4 | batch   150/ 3263 | ms/batch  4.03 | loss  4.88 | ppl   132.22\n",
      "| epoch   4 | batch   200/ 3263 | ms/batch  4.11 | loss  4.89 | ppl   132.70\n",
      "| epoch   4 | batch   250/ 3263 | ms/batch  4.16 | loss  4.90 | ppl   134.89\n",
      "| epoch   4 | batch   300/ 3263 | ms/batch  4.11 | loss  4.87 | ppl   130.16\n",
      "| epoch   4 | batch   350/ 3263 | ms/batch  4.17 | loss  4.91 | ppl   135.69\n",
      "| epoch   4 | batch   400/ 3263 | ms/batch  4.41 | loss  4.91 | ppl   136.28\n",
      "| epoch   4 | batch   450/ 3263 | ms/batch  4.03 | loss  4.95 | ppl   140.54\n",
      "| epoch   4 | batch   500/ 3263 | ms/batch  4.25 | loss  4.89 | ppl   132.80\n",
      "| epoch   4 | batch   550/ 3263 | ms/batch  4.27 | loss  4.92 | ppl   137.39\n",
      "| epoch   4 | batch   600/ 3263 | ms/batch  4.10 | loss  4.91 | ppl   136.20\n",
      "| epoch   4 | batch   650/ 3263 | ms/batch  4.41 | loss  4.90 | ppl   134.72\n",
      "| epoch   4 | batch   700/ 3263 | ms/batch  3.88 | loss  4.90 | ppl   133.70\n",
      "| epoch   4 | batch   750/ 3263 | ms/batch  3.91 | loss  4.90 | ppl   134.23\n",
      "| epoch   4 | batch   800/ 3263 | ms/batch  3.88 | loss  4.89 | ppl   132.69\n",
      "| epoch   4 | batch   850/ 3263 | ms/batch  3.93 | loss  4.87 | ppl   130.01\n",
      "| epoch   4 | batch   900/ 3263 | ms/batch  3.91 | loss  4.93 | ppl   138.00\n",
      "| epoch   4 | batch   950/ 3263 | ms/batch  3.88 | loss  4.90 | ppl   134.89\n",
      "| epoch   4 | batch  1000/ 3263 | ms/batch  3.89 | loss  4.94 | ppl   139.19\n",
      "| epoch   4 | batch  1050/ 3263 | ms/batch  3.91 | loss  4.83 | ppl   125.70\n",
      "| epoch   4 | batch  1100/ 3263 | ms/batch  3.94 | loss  4.93 | ppl   137.75\n",
      "| epoch   4 | batch  1150/ 3263 | ms/batch  3.93 | loss  4.94 | ppl   140.23\n",
      "| epoch   4 | batch  1200/ 3263 | ms/batch  3.90 | loss  4.92 | ppl   136.39\n",
      "| epoch   4 | batch  1250/ 3263 | ms/batch  4.00 | loss  4.90 | ppl   134.79\n",
      "| epoch   4 | batch  1300/ 3263 | ms/batch  3.90 | loss  4.92 | ppl   137.28\n",
      "| epoch   4 | batch  1350/ 3263 | ms/batch  3.91 | loss  4.93 | ppl   137.84\n",
      "| epoch   4 | batch  1400/ 3263 | ms/batch  3.90 | loss  4.95 | ppl   140.87\n",
      "| epoch   4 | batch  1450/ 3263 | ms/batch  3.90 | loss  4.91 | ppl   135.15\n",
      "| epoch   4 | batch  1500/ 3263 | ms/batch  3.89 | loss  4.95 | ppl   140.78\n",
      "| epoch   4 | batch  1550/ 3263 | ms/batch  3.90 | loss  4.93 | ppl   138.95\n",
      "| epoch   4 | batch  1600/ 3263 | ms/batch  3.91 | loss  4.98 | ppl   145.70\n",
      "| epoch   4 | batch  1650/ 3263 | ms/batch  3.91 | loss  4.95 | ppl   141.42\n",
      "| epoch   4 | batch  1700/ 3263 | ms/batch  3.88 | loss  4.90 | ppl   134.10\n",
      "| epoch   4 | batch  1750/ 3263 | ms/batch  3.89 | loss  4.89 | ppl   133.09\n",
      "| epoch   4 | batch  1800/ 3263 | ms/batch  3.91 | loss  4.91 | ppl   136.20\n",
      "| epoch   4 | batch  1850/ 3263 | ms/batch  3.91 | loss  4.94 | ppl   139.40\n",
      "| epoch   4 | batch  1900/ 3263 | ms/batch  3.89 | loss  4.91 | ppl   135.02\n",
      "| epoch   4 | batch  1950/ 3263 | ms/batch  3.89 | loss  4.93 | ppl   138.18\n",
      "| epoch   4 | batch  2000/ 3263 | ms/batch  3.91 | loss  4.94 | ppl   140.37\n",
      "| epoch   4 | batch  2050/ 3263 | ms/batch  3.90 | loss  4.99 | ppl   146.74\n",
      "| epoch   4 | batch  2100/ 3263 | ms/batch  3.94 | loss  4.96 | ppl   142.71\n",
      "| epoch   4 | batch  2150/ 3263 | ms/batch  4.19 | loss  4.95 | ppl   141.49\n",
      "| epoch   4 | batch  2200/ 3263 | ms/batch  4.14 | loss  4.93 | ppl   139.00\n",
      "| epoch   4 | batch  2250/ 3263 | ms/batch  4.37 | loss  4.86 | ppl   129.37\n",
      "| epoch   4 | batch  2300/ 3263 | ms/batch  4.19 | loss  4.91 | ppl   135.61\n",
      "| epoch   4 | batch  2350/ 3263 | ms/batch  4.17 | loss  4.85 | ppl   127.70\n",
      "| epoch   4 | batch  2400/ 3263 | ms/batch  4.16 | loss  4.87 | ppl   129.82\n",
      "| epoch   4 | batch  2450/ 3263 | ms/batch  4.11 | loss  4.87 | ppl   130.39\n",
      "| epoch   4 | batch  2500/ 3263 | ms/batch  3.92 | loss  4.88 | ppl   131.25\n",
      "| epoch   4 | batch  2550/ 3263 | ms/batch  3.91 | loss  4.86 | ppl   129.44\n",
      "| epoch   4 | batch  2600/ 3263 | ms/batch  3.90 | loss  4.89 | ppl   132.70\n",
      "| epoch   4 | batch  2650/ 3263 | ms/batch  3.91 | loss  4.86 | ppl   128.81\n",
      "| epoch   4 | batch  2700/ 3263 | ms/batch  3.87 | loss  4.91 | ppl   135.85\n",
      "| epoch   4 | batch  2750/ 3263 | ms/batch  3.90 | loss  4.92 | ppl   136.93\n",
      "| epoch   4 | batch  2800/ 3263 | ms/batch  3.89 | loss  4.99 | ppl   146.49\n",
      "| epoch   4 | batch  2850/ 3263 | ms/batch  3.89 | loss  4.92 | ppl   136.40\n",
      "| epoch   4 | batch  2900/ 3263 | ms/batch  3.88 | loss  4.90 | ppl   134.67\n",
      "| epoch   4 | batch  2950/ 3263 | ms/batch  3.89 | loss  4.93 | ppl   138.24\n",
      "| epoch   4 | batch  3000/ 3263 | ms/batch  3.88 | loss  4.91 | ppl   135.23\n",
      "| epoch   4 | batch  3050/ 3263 | ms/batch  3.89 | loss  4.89 | ppl   132.97\n",
      "| epoch   4 | batch  3100/ 3263 | ms/batch  3.90 | loss  4.89 | ppl   132.84\n",
      "| epoch   4 | batch  3150/ 3263 | ms/batch  3.91 | loss  4.92 | ppl   136.41\n",
      "| epoch   4 | batch  3200/ 3263 | ms/batch  3.88 | loss  4.93 | ppl   138.38\n",
      "| epoch   4 | batch  3250/ 3263 | ms/batch  3.91 | loss  4.88 | ppl   132.00\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 13.33s | valid ppl   119.50\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   119.50\n",
      "| epoch   5 | batch    50/ 3263 | ms/batch  3.99 | loss  4.98 | ppl   145.48\n",
      "| epoch   5 | batch   100/ 3263 | ms/batch  3.88 | loss  4.87 | ppl   130.04\n",
      "| epoch   5 | batch   150/ 3263 | ms/batch  4.10 | loss  4.82 | ppl   124.09\n",
      "| epoch   5 | batch   200/ 3263 | ms/batch  4.19 | loss  4.83 | ppl   125.36\n",
      "| epoch   5 | batch   250/ 3263 | ms/batch  4.09 | loss  4.83 | ppl   125.78\n",
      "| epoch   5 | batch   300/ 3263 | ms/batch  3.89 | loss  4.80 | ppl   121.75\n",
      "| epoch   5 | batch   350/ 3263 | ms/batch  3.89 | loss  4.84 | ppl   127.06\n",
      "| epoch   5 | batch   400/ 3263 | ms/batch  3.90 | loss  4.85 | ppl   127.10\n",
      "| epoch   5 | batch   450/ 3263 | ms/batch  3.88 | loss  4.91 | ppl   136.05\n",
      "| epoch   5 | batch   500/ 3263 | ms/batch  3.92 | loss  4.83 | ppl   124.59\n",
      "| epoch   5 | batch   550/ 3263 | ms/batch  3.90 | loss  4.85 | ppl   128.21\n",
      "| epoch   5 | batch   600/ 3263 | ms/batch  3.88 | loss  4.85 | ppl   128.16\n",
      "| epoch   5 | batch   650/ 3263 | ms/batch  3.97 | loss  4.84 | ppl   126.56\n",
      "| epoch   5 | batch   700/ 3263 | ms/batch  4.06 | loss  4.84 | ppl   126.62\n",
      "| epoch   5 | batch   750/ 3263 | ms/batch  3.90 | loss  4.83 | ppl   125.69\n",
      "| epoch   5 | batch   800/ 3263 | ms/batch  3.90 | loss  4.83 | ppl   124.92\n",
      "| epoch   5 | batch   850/ 3263 | ms/batch  3.89 | loss  4.82 | ppl   124.46\n",
      "| epoch   5 | batch   900/ 3263 | ms/batch  3.90 | loss  4.88 | ppl   131.65\n",
      "| epoch   5 | batch   950/ 3263 | ms/batch  3.88 | loss  4.85 | ppl   127.16\n",
      "| epoch   5 | batch  1000/ 3263 | ms/batch  3.92 | loss  4.89 | ppl   132.62\n",
      "| epoch   5 | batch  1050/ 3263 | ms/batch  3.89 | loss  4.79 | ppl   120.27\n",
      "| epoch   5 | batch  1100/ 3263 | ms/batch  3.90 | loss  4.87 | ppl   129.86\n",
      "| epoch   5 | batch  1150/ 3263 | ms/batch  3.90 | loss  4.87 | ppl   130.66\n",
      "| epoch   5 | batch  1200/ 3263 | ms/batch  3.89 | loss  4.86 | ppl   128.91\n",
      "| epoch   5 | batch  1250/ 3263 | ms/batch  3.90 | loss  4.85 | ppl   127.47\n",
      "| epoch   5 | batch  1300/ 3263 | ms/batch  3.88 | loss  4.87 | ppl   130.68\n",
      "| epoch   5 | batch  1350/ 3263 | ms/batch  3.91 | loss  4.87 | ppl   130.40\n",
      "| epoch   5 | batch  1400/ 3263 | ms/batch  3.88 | loss  4.89 | ppl   133.54\n",
      "| epoch   5 | batch  1450/ 3263 | ms/batch  3.91 | loss  4.86 | ppl   129.55\n",
      "| epoch   5 | batch  1500/ 3263 | ms/batch  3.91 | loss  4.88 | ppl   131.47\n",
      "| epoch   5 | batch  1550/ 3263 | ms/batch  3.88 | loss  4.87 | ppl   130.77\n",
      "| epoch   5 | batch  1600/ 3263 | ms/batch  3.91 | loss  4.92 | ppl   137.12\n",
      "| epoch   5 | batch  1650/ 3263 | ms/batch  3.89 | loss  4.90 | ppl   133.63\n",
      "| epoch   5 | batch  1700/ 3263 | ms/batch  3.95 | loss  4.83 | ppl   124.99\n",
      "| epoch   5 | batch  1750/ 3263 | ms/batch  3.91 | loss  4.84 | ppl   126.74\n",
      "| epoch   5 | batch  1800/ 3263 | ms/batch  3.92 | loss  4.87 | ppl   130.97\n",
      "| epoch   5 | batch  1850/ 3263 | ms/batch  3.89 | loss  4.89 | ppl   132.72\n",
      "| epoch   5 | batch  1900/ 3263 | ms/batch  3.90 | loss  4.84 | ppl   126.99\n",
      "| epoch   5 | batch  1950/ 3263 | ms/batch  3.91 | loss  4.88 | ppl   131.56\n",
      "| epoch   5 | batch  2000/ 3263 | ms/batch  4.01 | loss  4.91 | ppl   135.70\n",
      "| epoch   5 | batch  2050/ 3263 | ms/batch  3.92 | loss  4.95 | ppl   140.55\n",
      "| epoch   5 | batch  2100/ 3263 | ms/batch  3.90 | loss  4.91 | ppl   135.06\n",
      "| epoch   5 | batch  2150/ 3263 | ms/batch  3.89 | loss  4.92 | ppl   136.60\n",
      "| epoch   5 | batch  2200/ 3263 | ms/batch  3.90 | loss  4.89 | ppl   132.46\n",
      "| epoch   5 | batch  2250/ 3263 | ms/batch  3.92 | loss  4.82 | ppl   123.80\n",
      "| epoch   5 | batch  2300/ 3263 | ms/batch  3.91 | loss  4.85 | ppl   128.07\n",
      "| epoch   5 | batch  2350/ 3263 | ms/batch  3.89 | loss  4.79 | ppl   120.51\n",
      "| epoch   5 | batch  2400/ 3263 | ms/batch  3.90 | loss  4.82 | ppl   123.93\n",
      "| epoch   5 | batch  2450/ 3263 | ms/batch  3.90 | loss  4.82 | ppl   124.40\n",
      "| epoch   5 | batch  2500/ 3263 | ms/batch  3.91 | loss  4.83 | ppl   125.07\n",
      "| epoch   5 | batch  2550/ 3263 | ms/batch  3.90 | loss  4.81 | ppl   122.39\n",
      "| epoch   5 | batch  2600/ 3263 | ms/batch  3.91 | loss  4.84 | ppl   126.80\n",
      "| epoch   5 | batch  2650/ 3263 | ms/batch  3.93 | loss  4.81 | ppl   122.46\n",
      "| epoch   5 | batch  2700/ 3263 | ms/batch  3.99 | loss  4.87 | ppl   130.29\n",
      "| epoch   5 | batch  2750/ 3263 | ms/batch  4.10 | loss  4.87 | ppl   130.57\n",
      "| epoch   5 | batch  2800/ 3263 | ms/batch  4.19 | loss  4.95 | ppl   140.63\n",
      "| epoch   5 | batch  2850/ 3263 | ms/batch  4.50 | loss  4.87 | ppl   130.12\n",
      "| epoch   5 | batch  2900/ 3263 | ms/batch  4.15 | loss  4.86 | ppl   129.05\n",
      "| epoch   5 | batch  2950/ 3263 | ms/batch  4.52 | loss  4.88 | ppl   132.25\n",
      "| epoch   5 | batch  3000/ 3263 | ms/batch  3.72 | loss  4.84 | ppl   126.94\n",
      "| epoch   5 | batch  3050/ 3263 | ms/batch  3.82 | loss  4.84 | ppl   126.74\n",
      "| epoch   5 | batch  3100/ 3263 | ms/batch  3.80 | loss  4.84 | ppl   125.97\n",
      "| epoch   5 | batch  3150/ 3263 | ms/batch  3.82 | loss  4.87 | ppl   130.45\n",
      "| epoch   5 | batch  3200/ 3263 | ms/batch  3.90 | loss  4.89 | ppl   133.54\n",
      "| epoch   5 | batch  3250/ 3263 | ms/batch  4.17 | loss  4.85 | ppl   127.14\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 13.19s | valid ppl   118.39\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   118.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_315106/1713799519.py:24: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best_rnn_model.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================================================================================\n",
      "| End of training | test ppl   107.55\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "try:\n",
    "    best_val_ppl = float('inf')\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, train_data, criterion, optimizer, seq_length)\n",
    "        val_ppl = evaluate(model, valid_data, criterion, seq_length)\n",
    "        \n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {(time.time() - epoch_start_time):5.2f}s | '\n",
    "              f'valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "        \n",
    "        # Save the model if validation performance improves\n",
    "        if val_ppl < best_val_ppl:\n",
    "            best_val_ppl = val_ppl\n",
    "            torch.save(model.state_dict(), 'best_rnn_model.pth')\n",
    "            print(f\"New best model saved with perplexity: {val_ppl:8.2f}\")\n",
    "\n",
    "    # Load best model and evaluate on test set\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('best_rnn_model.pth'))\n",
    "        test_ppl = evaluate(model, test_data, criterion, seq_length)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test ppl {test_ppl:8.2f}')\n",
    "        print('=' * 89)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "        print(\"Evaluating with current model instead.\")\n",
    "        test_ppl = evaluate(model, test_data, criterion, seq_length)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test ppl {test_ppl:8.2f}')\n",
    "        print('=' * 89)\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text Sample:\n",
      "the <unk> , while both <unk> the 2010 , <unk> was H. Wright of the state of 1994 . Despite the <unk> , the characters were noted that it was not come for level in <unk> , modern years , though she told is commentators considered mainly in its <unk> itself\n"
     ]
    }
   ],
   "source": [
    "# Function to generate text\n",
    "def generate_text(model, vocab, seed_text=\"the\", max_length=50):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocab (index to token)\n",
    "    idx_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    # Convert seed text to tensor\n",
    "    if seed_text in vocab:\n",
    "        input_idx = vocab[seed_text]\n",
    "    else:\n",
    "        input_idx = vocab['<unk>']\n",
    "    \n",
    "    input_tensor = torch.tensor([[input_idx]], device=device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_tokens = [seed_text]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # Sample from the output distribution\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Add generated token to output\n",
    "            generated_tokens.append(idx_to_token.get(next_token_idx, '<unk>'))\n",
    "            \n",
    "            # Update input for next iteration\n",
    "            input_tensor = torch.tensor([[next_token_idx]], device=device)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n",
    "\n",
    "# Generate and print some text\n",
    "print(\"\\nGenerated Text Sample:\")\n",
    "print(generate_text(model, vocab, seed_text=\"the\", max_length=50))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
