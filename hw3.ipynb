{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/ben/.cache/kagglehub/datasets/bestwater/wikitext-2-v1/versions/1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bestwater/wikitext-2-v1\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**THEN MOVE THE FILE DIRECTORY TO THIS DIRECTORY**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\"\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(2)\n",
    "np.random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WikiText2 corpus paths\n",
    "corpus_path = './wikitext-2'\n",
    "train_path = os.path.join(corpus_path, 'wiki.train.tokens')\n",
    "valid_path = os.path.join(corpus_path, 'wiki.valid.tokens')\n",
    "test_path = os.path.join(corpus_path, 'wiki.test.tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 100\n",
    "hidden_dim = 256\n",
    "dropout_prob = 0.5\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "seq_length = 29  # Number of unrolled time steps\n",
    "learning_rate = 0.001\n",
    "vocab_size = 10000  # Reduced vocabulary size\n",
    "unk_threshold = 5  # Frequency threshold for unknown tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    \"\"\"Read corpus file and return list of whitespace-tokenized words\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().replace('\\n', ' <eos> ')\n",
    "    return text.split(' ')\n",
    "\n",
    "def build_vocab(tokens, threshold=unk_threshold):\n",
    "    \"\"\"Build vocabulary from tokens with frequency threshold\"\"\"\n",
    "    counter = Counter(tokens)\n",
    "    # Sort tokens by frequency (descending)\n",
    "    sorted_tokens = sorted(counter.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Create vocabulary: reserve 0 for padding, 1 for <unk>\n",
    "    vocab = {'<pad>': 0, '<unk>': 1, '<eos>': 2}\n",
    "    idx = 3\n",
    "    \n",
    "    # Add tokens that appear more than threshold times\n",
    "    for token, count in sorted_tokens:\n",
    "        if count >= threshold and idx < vocab_size:\n",
    "            if token and token != '<eos>':  # Skip empty tokens and already added special tokens\n",
    "                vocab[token] = idx\n",
    "                idx += 1\n",
    "        if idx >= vocab_size:\n",
    "            break\n",
    "    \n",
    "    return vocab\n",
    "\n",
    "def tokens_to_indices(tokens, vocab):\n",
    "    \"\"\"Convert tokens to indices using vocabulary\"\"\"\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens if token]\n",
    "\n",
    "def batchify(data, bsz):\n",
    "    \"\"\"Divide dataset into batches and arrange for back-propagation through time\"\"\"\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit\n",
    "    data = data[:nbatch * bsz]\n",
    "    # Evenly divide the data across the bsz batches\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "def get_batch(source, i, seq_length):\n",
    "    \"\"\"Get a batch for training\"\"\"\n",
    "    seq_len = min(seq_length, source.size(0) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, dropout_prob):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=False, dropout=dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.decoder = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        \"\"\"Initialize weights for better training\"\"\"\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialize hidden state\"\"\"\n",
    "        return torch.zeros(1, batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        # x shape: (seq_len, batch_size)\n",
    "        emb = self.dropout(self.embedding(x))  # (seq_len, batch_size, embed_dim)\n",
    "        output, hidden = self.rnn(emb, hidden)  # output: (seq_len, batch_size, hidden_dim)\n",
    "        output = self.dropout(output)\n",
    "        decoded = self.decoder(output.view(-1, self.hidden_dim))  # (seq_len*batch_size, vocab_size)\n",
    "        return decoded, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, criterion, optimizer, seq_length):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    \n",
    "    # Get total number of batches\n",
    "    num_batches = (train_data.size(0) - 1) // seq_length\n",
    "    \n",
    "    for batch, i in enumerate(range(0, train_data.size(0) - 1, seq_length)):\n",
    "        # Skip if we'd go out of bounds\n",
    "        if i > train_data.size(0) - 2:\n",
    "            continue\n",
    "            \n",
    "        data, targets = get_batch(train_data, i, seq_length)\n",
    "        \n",
    "        # Initialize hidden state for new batch\n",
    "        hidden = hidden.detach()\n",
    "        \n",
    "        # Forward pass\n",
    "        output, hidden = model(data, hidden)\n",
    "        loss = criterion(output, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        if batch % 50 == 0 and batch > 0:\n",
    "            cur_loss = total_loss / 50\n",
    "            elapsed = time.time() - start_time\n",
    "            print(f'| epoch {epoch:3d} | batch {batch:5d}/{num_batches:5d} | '\n",
    "                  f'ms/batch {elapsed * 1000 / 50:5.2f} | '\n",
    "                  f'loss {cur_loss:5.2f} | ppl {math.exp(cur_loss):8.2f}')\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "    \n",
    "def evaluate(model, eval_data, criterion, seq_length):\n",
    "    \"\"\"Evaluate the model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    num_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, eval_data.size(0) - 1, seq_length):\n",
    "            # Skip if we'd go out of bounds\n",
    "            if i > eval_data.size(0) - 2:\n",
    "                continue\n",
    "                \n",
    "            data, targets = get_batch(eval_data, i, seq_length)\n",
    "            hidden = hidden.detach()\n",
    "            output, hidden = model(data, hidden)\n",
    "            \n",
    "            loss = criterion(output, targets)\n",
    "            total_loss += loss.item() * targets.size(0)\n",
    "            num_tokens += targets.size(0)\n",
    "    \n",
    "    # Calculate perplexity\n",
    "    avg_loss = total_loss / num_tokens if num_tokens > 0 else float('inf')\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preprocessing corpus...\n",
      "Vocabulary size: 9999\n",
      "Warning: Found index 9999 but vocab size is 9999\n",
      "Fixing indices...\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load and preprocess data\n",
    "    print(\"Loading and preprocessing corpus...\")\n",
    "    train_tokens = read_corpus(train_path)\n",
    "    valid_tokens = read_corpus(valid_path)\n",
    "    test_tokens = read_corpus(test_path)\n",
    "\n",
    "    # Build vocabulary from training tokens\n",
    "    vocab = build_vocab(train_tokens)\n",
    "    print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    train_indices = tokens_to_indices(train_tokens, vocab)\n",
    "    valid_indices = tokens_to_indices(valid_tokens, vocab)\n",
    "    test_indices = tokens_to_indices(test_tokens, vocab)\n",
    "\n",
    "    # Safety check: ensure all indices are valid\n",
    "    max_idx = max(train_indices + valid_indices + test_indices)\n",
    "    if max_idx >= len(vocab):\n",
    "        print(f\"Warning: Found index {max_idx} but vocab size is {len(vocab)}\")\n",
    "        print(\"Fixing indices...\")\n",
    "        train_indices = [min(idx, len(vocab) - 1) for idx in train_indices]\n",
    "        valid_indices = [min(idx, len(vocab) - 1) for idx in valid_indices]\n",
    "        test_indices = [min(idx, len(vocab) - 1) for idx in test_indices]\n",
    "\n",
    "    # Convert to tensors and batchify - use CPU first for safety\n",
    "    train_data = batchify(torch.tensor(train_indices, dtype=torch.long), batch_size)\n",
    "    valid_data = batchify(torch.tensor(valid_indices, dtype=torch.long), batch_size)\n",
    "    test_data = batchify(torch.tensor(test_indices, dtype=torch.long), batch_size)\n",
    "\n",
    "    # Initialize model\n",
    "    model = RNNModel(len(vocab), embed_dim, hidden_dim, dropout_prob).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "| epoch   1 | batch    50/ 2250 | ms/batch  5.32 | loss  7.18 | ppl  1312.70\n",
      "| epoch   1 | batch   100/ 2250 | ms/batch  4.73 | loss  6.37 | ppl   582.17\n",
      "| epoch   1 | batch   150/ 2250 | ms/batch  4.77 | loss  6.14 | ppl   466.26\n",
      "| epoch   1 | batch   200/ 2250 | ms/batch  4.73 | loss  5.99 | ppl   399.39\n",
      "| epoch   1 | batch   250/ 2250 | ms/batch  4.72 | loss  5.95 | ppl   384.58\n",
      "| epoch   1 | batch   300/ 2250 | ms/batch  4.72 | loss  5.95 | ppl   383.56\n",
      "| epoch   1 | batch   350/ 2250 | ms/batch  4.74 | loss  5.83 | ppl   341.99\n",
      "| epoch   1 | batch   400/ 2250 | ms/batch  4.72 | loss  5.80 | ppl   329.88\n",
      "| epoch   1 | batch   450/ 2250 | ms/batch  4.71 | loss  5.72 | ppl   303.80\n",
      "| epoch   1 | batch   500/ 2250 | ms/batch  4.71 | loss  5.69 | ppl   295.95\n",
      "| epoch   1 | batch   550/ 2250 | ms/batch  4.71 | loss  5.63 | ppl   277.63\n",
      "| epoch   1 | batch   600/ 2250 | ms/batch  4.71 | loss  5.60 | ppl   269.09\n",
      "| epoch   1 | batch   650/ 2250 | ms/batch  4.73 | loss  5.58 | ppl   266.04\n",
      "| epoch   1 | batch   700/ 2250 | ms/batch  4.93 | loss  5.61 | ppl   274.05\n",
      "| epoch   1 | batch   750/ 2250 | ms/batch  4.81 | loss  5.60 | ppl   270.61\n",
      "| epoch   1 | batch   800/ 2250 | ms/batch  4.72 | loss  5.63 | ppl   277.58\n",
      "| epoch   1 | batch   850/ 2250 | ms/batch  4.73 | loss  5.49 | ppl   241.22\n",
      "| epoch   1 | batch   900/ 2250 | ms/batch  4.76 | loss  5.54 | ppl   254.34\n",
      "| epoch   1 | batch   950/ 2250 | ms/batch  4.83 | loss  5.54 | ppl   255.77\n",
      "| epoch   1 | batch  1000/ 2250 | ms/batch  4.72 | loss  5.52 | ppl   250.86\n",
      "| epoch   1 | batch  1050/ 2250 | ms/batch  4.72 | loss  5.52 | ppl   249.41\n",
      "| epoch   1 | batch  1100/ 2250 | ms/batch  5.00 | loss  5.54 | ppl   255.08\n",
      "| epoch   1 | batch  1150/ 2250 | ms/batch  4.94 | loss  5.51 | ppl   248.22\n",
      "| epoch   1 | batch  1200/ 2250 | ms/batch  4.95 | loss  5.43 | ppl   227.45\n",
      "| epoch   1 | batch  1250/ 2250 | ms/batch  4.97 | loss  5.46 | ppl   234.49\n",
      "| epoch   1 | batch  1300/ 2250 | ms/batch  4.98 | loss  5.42 | ppl   226.95\n",
      "| epoch   1 | batch  1350/ 2250 | ms/batch  5.02 | loss  5.40 | ppl   222.22\n",
      "| epoch   1 | batch  1400/ 2250 | ms/batch  5.04 | loss  5.45 | ppl   233.01\n",
      "| epoch   1 | batch  1450/ 2250 | ms/batch  5.01 | loss  5.44 | ppl   229.67\n",
      "| epoch   1 | batch  1500/ 2250 | ms/batch  4.95 | loss  5.43 | ppl   228.99\n",
      "| epoch   1 | batch  1550/ 2250 | ms/batch  4.96 | loss  5.40 | ppl   221.54\n",
      "| epoch   1 | batch  1600/ 2250 | ms/batch  4.95 | loss  5.41 | ppl   222.72\n",
      "| epoch   1 | batch  1650/ 2250 | ms/batch  4.99 | loss  5.37 | ppl   214.17\n",
      "| epoch   1 | batch  1700/ 2250 | ms/batch  4.96 | loss  5.38 | ppl   217.67\n",
      "| epoch   1 | batch  1750/ 2250 | ms/batch  4.94 | loss  5.36 | ppl   211.91\n",
      "| epoch   1 | batch  1800/ 2250 | ms/batch  4.98 | loss  5.34 | ppl   207.73\n",
      "| epoch   1 | batch  1850/ 2250 | ms/batch  4.98 | loss  5.35 | ppl   210.37\n",
      "| epoch   1 | batch  1900/ 2250 | ms/batch  5.00 | loss  5.40 | ppl   221.19\n",
      "| epoch   1 | batch  1950/ 2250 | ms/batch  5.00 | loss  5.40 | ppl   222.48\n",
      "| epoch   1 | batch  2000/ 2250 | ms/batch  5.01 | loss  5.34 | ppl   209.38\n",
      "| epoch   1 | batch  2050/ 2250 | ms/batch  5.02 | loss  5.37 | ppl   214.99\n",
      "| epoch   1 | batch  2100/ 2250 | ms/batch  5.17 | loss  5.30 | ppl   200.32\n",
      "| epoch   1 | batch  2150/ 2250 | ms/batch  4.96 | loss  5.30 | ppl   199.83\n",
      "| epoch   1 | batch  2200/ 2250 | ms/batch  4.97 | loss  5.30 | ppl   199.92\n",
      "| epoch   1 | batch  2250/ 2250 | ms/batch  5.07 | loss  5.24 | ppl   188.51\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 13.98s | valid ppl   155.64\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   155.64\n",
      "| epoch   2 | batch    50/ 2250 | ms/batch  5.08 | loss  5.33 | ppl   205.45\n",
      "| epoch   2 | batch   100/ 2250 | ms/batch  4.98 | loss  5.22 | ppl   184.14\n",
      "| epoch   2 | batch   150/ 2250 | ms/batch  4.97 | loss  5.20 | ppl   180.52\n",
      "| epoch   2 | batch   200/ 2250 | ms/batch  4.98 | loss  5.19 | ppl   178.79\n",
      "| epoch   2 | batch   250/ 2250 | ms/batch  4.99 | loss  5.20 | ppl   181.40\n",
      "| epoch   2 | batch   300/ 2250 | ms/batch  4.99 | loss  5.27 | ppl   194.53\n",
      "| epoch   2 | batch   350/ 2250 | ms/batch  4.90 | loss  5.20 | ppl   181.07\n",
      "| epoch   2 | batch   400/ 2250 | ms/batch  4.80 | loss  5.22 | ppl   184.08\n",
      "| epoch   2 | batch   450/ 2250 | ms/batch  4.78 | loss  5.16 | ppl   174.96\n",
      "| epoch   2 | batch   500/ 2250 | ms/batch  4.76 | loss  5.15 | ppl   172.05\n",
      "| epoch   2 | batch   550/ 2250 | ms/batch  4.77 | loss  5.14 | ppl   170.95\n",
      "| epoch   2 | batch   600/ 2250 | ms/batch  4.74 | loss  5.14 | ppl   170.80\n",
      "| epoch   2 | batch   650/ 2250 | ms/batch  4.78 | loss  5.17 | ppl   175.43\n",
      "| epoch   2 | batch   700/ 2250 | ms/batch  4.75 | loss  5.18 | ppl   177.47\n",
      "| epoch   2 | batch   750/ 2250 | ms/batch  4.77 | loss  5.16 | ppl   173.38\n",
      "| epoch   2 | batch   800/ 2250 | ms/batch  4.78 | loss  5.22 | ppl   184.06\n",
      "| epoch   2 | batch   850/ 2250 | ms/batch  4.97 | loss  5.12 | ppl   166.75\n",
      "| epoch   2 | batch   900/ 2250 | ms/batch  4.96 | loss  5.17 | ppl   175.86\n",
      "| epoch   2 | batch   950/ 2250 | ms/batch  5.04 | loss  5.18 | ppl   177.65\n",
      "| epoch   2 | batch  1000/ 2250 | ms/batch  4.97 | loss  5.16 | ppl   174.93\n",
      "| epoch   2 | batch  1050/ 2250 | ms/batch  5.01 | loss  5.17 | ppl   176.78\n",
      "| epoch   2 | batch  1100/ 2250 | ms/batch  5.01 | loss  5.20 | ppl   181.07\n",
      "| epoch   2 | batch  1150/ 2250 | ms/batch  4.84 | loss  5.20 | ppl   181.18\n",
      "| epoch   2 | batch  1200/ 2250 | ms/batch  4.78 | loss  5.12 | ppl   166.70\n",
      "| epoch   2 | batch  1250/ 2250 | ms/batch  4.78 | loss  5.15 | ppl   171.82\n",
      "| epoch   2 | batch  1300/ 2250 | ms/batch  4.77 | loss  5.14 | ppl   170.55\n",
      "| epoch   2 | batch  1350/ 2250 | ms/batch  4.77 | loss  5.13 | ppl   168.20\n",
      "| epoch   2 | batch  1400/ 2250 | ms/batch  4.73 | loss  5.17 | ppl   176.46\n",
      "| epoch   2 | batch  1450/ 2250 | ms/batch  4.74 | loss  5.17 | ppl   176.68\n",
      "| epoch   2 | batch  1500/ 2250 | ms/batch  4.74 | loss  5.16 | ppl   174.80\n",
      "| epoch   2 | batch  1550/ 2250 | ms/batch  4.72 | loss  5.09 | ppl   163.02\n",
      "| epoch   2 | batch  1600/ 2250 | ms/batch  4.94 | loss  5.11 | ppl   165.97\n",
      "| epoch   2 | batch  1650/ 2250 | ms/batch  5.00 | loss  5.09 | ppl   162.12\n",
      "| epoch   2 | batch  1700/ 2250 | ms/batch  5.01 | loss  5.10 | ppl   163.24\n",
      "| epoch   2 | batch  1750/ 2250 | ms/batch  5.08 | loss  5.08 | ppl   160.77\n",
      "| epoch   2 | batch  1800/ 2250 | ms/batch  5.01 | loss  5.07 | ppl   159.16\n",
      "| epoch   2 | batch  1850/ 2250 | ms/batch  4.98 | loss  5.08 | ppl   161.32\n",
      "| epoch   2 | batch  1900/ 2250 | ms/batch  4.99 | loss  5.13 | ppl   169.67\n",
      "| epoch   2 | batch  1950/ 2250 | ms/batch  5.25 | loss  5.16 | ppl   173.55\n",
      "| epoch   2 | batch  2000/ 2250 | ms/batch  5.04 | loss  5.10 | ppl   163.89\n",
      "| epoch   2 | batch  2050/ 2250 | ms/batch  5.08 | loss  5.12 | ppl   168.04\n",
      "| epoch   2 | batch  2100/ 2250 | ms/batch  4.96 | loss  5.08 | ppl   160.32\n",
      "| epoch   2 | batch  2150/ 2250 | ms/batch  4.85 | loss  5.08 | ppl   160.37\n",
      "| epoch   2 | batch  2200/ 2250 | ms/batch  4.96 | loss  5.09 | ppl   162.73\n",
      "| epoch   2 | batch  2250/ 2250 | ms/batch  4.75 | loss  5.06 | ppl   158.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 14.00s | valid ppl   132.61\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   132.61\n",
      "| epoch   3 | batch    50/ 2250 | ms/batch  5.09 | loss  5.15 | ppl   172.61\n",
      "| epoch   3 | batch   100/ 2250 | ms/batch  5.17 | loss  5.02 | ppl   151.69\n",
      "| epoch   3 | batch   150/ 2250 | ms/batch  5.03 | loss  5.00 | ppl   148.22\n",
      "| epoch   3 | batch   200/ 2250 | ms/batch  5.03 | loss  5.01 | ppl   149.69\n",
      "| epoch   3 | batch   250/ 2250 | ms/batch  5.08 | loss  5.01 | ppl   149.88\n",
      "| epoch   3 | batch   300/ 2250 | ms/batch  5.03 | loss  5.08 | ppl   161.11\n",
      "| epoch   3 | batch   350/ 2250 | ms/batch  4.97 | loss  5.02 | ppl   151.37\n",
      "| epoch   3 | batch   400/ 2250 | ms/batch  5.01 | loss  5.04 | ppl   154.09\n",
      "| epoch   3 | batch   450/ 2250 | ms/batch  5.00 | loss  5.00 | ppl   149.10\n",
      "| epoch   3 | batch   500/ 2250 | ms/batch  5.20 | loss  5.00 | ppl   147.82\n",
      "| epoch   3 | batch   550/ 2250 | ms/batch  5.13 | loss  5.00 | ppl   147.81\n",
      "| epoch   3 | batch   600/ 2250 | ms/batch  4.98 | loss  4.99 | ppl   147.57\n",
      "| epoch   3 | batch   650/ 2250 | ms/batch  4.98 | loss  5.01 | ppl   149.45\n",
      "| epoch   3 | batch   700/ 2250 | ms/batch  5.09 | loss  5.02 | ppl   150.87\n",
      "| epoch   3 | batch   750/ 2250 | ms/batch  5.13 | loss  4.99 | ppl   147.00\n",
      "| epoch   3 | batch   800/ 2250 | ms/batch  5.21 | loss  5.05 | ppl   156.17\n",
      "| epoch   3 | batch   850/ 2250 | ms/batch  4.98 | loss  4.98 | ppl   145.04\n",
      "| epoch   3 | batch   900/ 2250 | ms/batch  5.00 | loss  5.03 | ppl   152.89\n",
      "| epoch   3 | batch   950/ 2250 | ms/batch  4.96 | loss  5.04 | ppl   154.07\n",
      "| epoch   3 | batch  1000/ 2250 | ms/batch  5.00 | loss  5.02 | ppl   151.14\n",
      "| epoch   3 | batch  1050/ 2250 | ms/batch  4.98 | loss  5.02 | ppl   151.31\n",
      "| epoch   3 | batch  1100/ 2250 | ms/batch  5.04 | loss  5.06 | ppl   157.08\n",
      "| epoch   3 | batch  1150/ 2250 | ms/batch  5.03 | loss  5.06 | ppl   157.71\n",
      "| epoch   3 | batch  1200/ 2250 | ms/batch  5.38 | loss  4.97 | ppl   143.84\n",
      "| epoch   3 | batch  1250/ 2250 | ms/batch  5.01 | loss  5.00 | ppl   148.65\n",
      "| epoch   3 | batch  1300/ 2250 | ms/batch  5.26 | loss  5.00 | ppl   148.37\n",
      "| epoch   3 | batch  1350/ 2250 | ms/batch  5.11 | loss  4.99 | ppl   146.65\n",
      "| epoch   3 | batch  1400/ 2250 | ms/batch  4.86 | loss  5.05 | ppl   156.75\n",
      "| epoch   3 | batch  1450/ 2250 | ms/batch  4.97 | loss  5.05 | ppl   155.80\n",
      "| epoch   3 | batch  1500/ 2250 | ms/batch  5.00 | loss  5.04 | ppl   155.08\n",
      "| epoch   3 | batch  1550/ 2250 | ms/batch  4.98 | loss  4.97 | ppl   144.52\n",
      "| epoch   3 | batch  1600/ 2250 | ms/batch  5.29 | loss  4.97 | ppl   143.45\n",
      "| epoch   3 | batch  1650/ 2250 | ms/batch  5.22 | loss  4.95 | ppl   140.79\n",
      "| epoch   3 | batch  1700/ 2250 | ms/batch  5.06 | loss  4.96 | ppl   141.92\n",
      "| epoch   3 | batch  1750/ 2250 | ms/batch  4.99 | loss  4.95 | ppl   141.73\n",
      "| epoch   3 | batch  1800/ 2250 | ms/batch  5.05 | loss  4.95 | ppl   141.63\n",
      "| epoch   3 | batch  1850/ 2250 | ms/batch  4.95 | loss  4.97 | ppl   143.94\n",
      "| epoch   3 | batch  1900/ 2250 | ms/batch  5.12 | loss  5.01 | ppl   150.27\n",
      "| epoch   3 | batch  1950/ 2250 | ms/batch  5.00 | loss  5.05 | ppl   155.50\n",
      "| epoch   3 | batch  2000/ 2250 | ms/batch  4.97 | loss  4.99 | ppl   146.96\n",
      "| epoch   3 | batch  2050/ 2250 | ms/batch  4.97 | loss  5.01 | ppl   149.23\n",
      "| epoch   3 | batch  2100/ 2250 | ms/batch  4.98 | loss  4.97 | ppl   144.03\n",
      "| epoch   3 | batch  2150/ 2250 | ms/batch  4.99 | loss  4.96 | ppl   142.64\n",
      "| epoch   3 | batch  2200/ 2250 | ms/batch  5.03 | loss  4.99 | ppl   146.97\n",
      "| epoch   3 | batch  2250/ 2250 | ms/batch  5.13 | loss  4.97 | ppl   143.87\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 14.34s | valid ppl   125.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   125.03\n",
      "| epoch   4 | batch    50/ 2250 | ms/batch  5.57 | loss  5.04 | ppl   154.29\n",
      "| epoch   4 | batch   100/ 2250 | ms/batch  4.96 | loss  4.92 | ppl   136.88\n",
      "| epoch   4 | batch   150/ 2250 | ms/batch  4.95 | loss  4.90 | ppl   134.21\n",
      "| epoch   4 | batch   200/ 2250 | ms/batch  4.95 | loss  4.89 | ppl   133.14\n",
      "| epoch   4 | batch   250/ 2250 | ms/batch  4.97 | loss  4.90 | ppl   134.92\n",
      "| epoch   4 | batch   300/ 2250 | ms/batch  4.98 | loss  4.98 | ppl   145.92\n",
      "| epoch   4 | batch   350/ 2250 | ms/batch  4.99 | loss  4.92 | ppl   137.37\n",
      "| epoch   4 | batch   400/ 2250 | ms/batch  5.00 | loss  4.93 | ppl   138.16\n",
      "| epoch   4 | batch   450/ 2250 | ms/batch  5.13 | loss  4.90 | ppl   134.61\n",
      "| epoch   4 | batch   500/ 2250 | ms/batch  5.03 | loss  4.89 | ppl   133.11\n",
      "| epoch   4 | batch   550/ 2250 | ms/batch  5.03 | loss  4.91 | ppl   135.05\n",
      "| epoch   4 | batch   600/ 2250 | ms/batch  4.85 | loss  4.91 | ppl   135.88\n",
      "| epoch   4 | batch   650/ 2250 | ms/batch  4.84 | loss  4.91 | ppl   135.62\n",
      "| epoch   4 | batch   700/ 2250 | ms/batch  4.81 | loss  4.91 | ppl   136.13\n",
      "| epoch   4 | batch   750/ 2250 | ms/batch  4.79 | loss  4.89 | ppl   133.35\n",
      "| epoch   4 | batch   800/ 2250 | ms/batch  4.78 | loss  4.96 | ppl   142.89\n",
      "| epoch   4 | batch   850/ 2250 | ms/batch  4.79 | loss  4.91 | ppl   135.04\n",
      "| epoch   4 | batch   900/ 2250 | ms/batch  4.78 | loss  4.94 | ppl   139.71\n",
      "| epoch   4 | batch   950/ 2250 | ms/batch  4.75 | loss  4.94 | ppl   140.39\n",
      "| epoch   4 | batch  1000/ 2250 | ms/batch  4.74 | loss  4.93 | ppl   139.02\n",
      "| epoch   4 | batch  1050/ 2250 | ms/batch  4.73 | loss  4.95 | ppl   140.84\n",
      "| epoch   4 | batch  1100/ 2250 | ms/batch  4.75 | loss  4.96 | ppl   143.04\n",
      "| epoch   4 | batch  1150/ 2250 | ms/batch  4.92 | loss  4.99 | ppl   146.74\n",
      "| epoch   4 | batch  1200/ 2250 | ms/batch  5.04 | loss  4.89 | ppl   132.58\n",
      "| epoch   4 | batch  1250/ 2250 | ms/batch  4.98 | loss  4.91 | ppl   135.43\n",
      "| epoch   4 | batch  1300/ 2250 | ms/batch  5.12 | loss  4.92 | ppl   137.44\n",
      "| epoch   4 | batch  1350/ 2250 | ms/batch  5.02 | loss  4.93 | ppl   137.75\n",
      "| epoch   4 | batch  1400/ 2250 | ms/batch  5.00 | loss  4.97 | ppl   143.71\n",
      "| epoch   4 | batch  1450/ 2250 | ms/batch  5.01 | loss  4.97 | ppl   144.64\n",
      "| epoch   4 | batch  1500/ 2250 | ms/batch  4.96 | loss  4.98 | ppl   144.89\n",
      "| epoch   4 | batch  1550/ 2250 | ms/batch  4.76 | loss  4.90 | ppl   134.29\n",
      "| epoch   4 | batch  1600/ 2250 | ms/batch  4.80 | loss  4.89 | ppl   133.48\n",
      "| epoch   4 | batch  1650/ 2250 | ms/batch  4.80 | loss  4.87 | ppl   130.83\n",
      "| epoch   4 | batch  1700/ 2250 | ms/batch  4.77 | loss  4.87 | ppl   130.47\n",
      "| epoch   4 | batch  1750/ 2250 | ms/batch  4.97 | loss  4.88 | ppl   131.24\n",
      "| epoch   4 | batch  1800/ 2250 | ms/batch  4.95 | loss  4.88 | ppl   131.33\n",
      "| epoch   4 | batch  1850/ 2250 | ms/batch  5.00 | loss  4.90 | ppl   134.10\n",
      "| epoch   4 | batch  1900/ 2250 | ms/batch  5.01 | loss  4.94 | ppl   139.66\n",
      "| epoch   4 | batch  1950/ 2250 | ms/batch  5.01 | loss  4.97 | ppl   143.99\n",
      "| epoch   4 | batch  2000/ 2250 | ms/batch  5.04 | loss  4.92 | ppl   137.45\n",
      "| epoch   4 | batch  2050/ 2250 | ms/batch  5.04 | loss  4.93 | ppl   138.01\n",
      "| epoch   4 | batch  2100/ 2250 | ms/batch  5.02 | loss  4.90 | ppl   134.88\n",
      "| epoch   4 | batch  2150/ 2250 | ms/batch  5.04 | loss  4.89 | ppl   133.39\n",
      "| epoch   4 | batch  2200/ 2250 | ms/batch  5.05 | loss  4.94 | ppl   139.21\n",
      "| epoch   4 | batch  2250/ 2250 | ms/batch  5.14 | loss  4.90 | ppl   134.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 14.11s | valid ppl   119.42\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   119.42\n",
      "| epoch   5 | batch    50/ 2250 | ms/batch  5.10 | loss  4.99 | ppl   146.75\n",
      "| epoch   5 | batch   100/ 2250 | ms/batch  4.73 | loss  4.85 | ppl   127.87\n",
      "| epoch   5 | batch   150/ 2250 | ms/batch  4.73 | loss  4.83 | ppl   125.26\n",
      "| epoch   5 | batch   200/ 2250 | ms/batch  4.73 | loss  4.82 | ppl   124.51\n",
      "| epoch   5 | batch   250/ 2250 | ms/batch  4.90 | loss  4.84 | ppl   127.06\n",
      "| epoch   5 | batch   300/ 2250 | ms/batch  5.02 | loss  4.91 | ppl   135.82\n",
      "| epoch   5 | batch   350/ 2250 | ms/batch  4.97 | loss  4.85 | ppl   127.26\n",
      "| epoch   5 | batch   400/ 2250 | ms/batch  4.99 | loss  4.86 | ppl   129.31\n",
      "| epoch   5 | batch   450/ 2250 | ms/batch  4.99 | loss  4.85 | ppl   127.85\n",
      "| epoch   5 | batch   500/ 2250 | ms/batch  4.85 | loss  4.83 | ppl   125.18\n",
      "| epoch   5 | batch   550/ 2250 | ms/batch  5.03 | loss  4.86 | ppl   128.65\n",
      "| epoch   5 | batch   600/ 2250 | ms/batch  4.86 | loss  4.85 | ppl   127.86\n",
      "| epoch   5 | batch   650/ 2250 | ms/batch  4.81 | loss  4.85 | ppl   127.95\n",
      "| epoch   5 | batch   700/ 2250 | ms/batch  4.78 | loss  4.86 | ppl   128.67\n",
      "| epoch   5 | batch   750/ 2250 | ms/batch  4.78 | loss  4.83 | ppl   124.93\n",
      "| epoch   5 | batch   800/ 2250 | ms/batch  4.76 | loss  4.89 | ppl   133.51\n",
      "| epoch   5 | batch   850/ 2250 | ms/batch  4.77 | loss  4.84 | ppl   126.65\n",
      "| epoch   5 | batch   900/ 2250 | ms/batch  4.76 | loss  4.88 | ppl   132.13\n",
      "| epoch   5 | batch   950/ 2250 | ms/batch  4.75 | loss  4.88 | ppl   132.14\n",
      "| epoch   5 | batch  1000/ 2250 | ms/batch  4.95 | loss  4.88 | ppl   131.13\n",
      "| epoch   5 | batch  1050/ 2250 | ms/batch  5.27 | loss  4.88 | ppl   131.19\n",
      "| epoch   5 | batch  1100/ 2250 | ms/batch  5.04 | loss  4.92 | ppl   136.86\n",
      "| epoch   5 | batch  1150/ 2250 | ms/batch  5.29 | loss  4.93 | ppl   137.80\n",
      "| epoch   5 | batch  1200/ 2250 | ms/batch  5.09 | loss  4.84 | ppl   126.66\n",
      "| epoch   5 | batch  1250/ 2250 | ms/batch  4.96 | loss  4.86 | ppl   129.29\n",
      "| epoch   5 | batch  1300/ 2250 | ms/batch  4.98 | loss  4.87 | ppl   130.29\n",
      "| epoch   5 | batch  1350/ 2250 | ms/batch  4.99 | loss  4.87 | ppl   130.30\n",
      "| epoch   5 | batch  1400/ 2250 | ms/batch  5.06 | loss  4.93 | ppl   137.89\n",
      "| epoch   5 | batch  1450/ 2250 | ms/batch  5.01 | loss  4.91 | ppl   135.56\n",
      "| epoch   5 | batch  1500/ 2250 | ms/batch  5.03 | loss  4.93 | ppl   138.14\n",
      "| epoch   5 | batch  1550/ 2250 | ms/batch  5.01 | loss  4.84 | ppl   127.06\n",
      "| epoch   5 | batch  1600/ 2250 | ms/batch  5.00 | loss  4.84 | ppl   126.62\n",
      "| epoch   5 | batch  1650/ 2250 | ms/batch  4.98 | loss  4.81 | ppl   122.88\n",
      "| epoch   5 | batch  1700/ 2250 | ms/batch  4.99 | loss  4.82 | ppl   123.56\n",
      "| epoch   5 | batch  1750/ 2250 | ms/batch  5.09 | loss  4.82 | ppl   123.92\n",
      "| epoch   5 | batch  1800/ 2250 | ms/batch  5.07 | loss  4.82 | ppl   124.17\n",
      "| epoch   5 | batch  1850/ 2250 | ms/batch  4.95 | loss  4.83 | ppl   124.90\n",
      "| epoch   5 | batch  1900/ 2250 | ms/batch  5.02 | loss  4.88 | ppl   130.98\n",
      "| epoch   5 | batch  1950/ 2250 | ms/batch  4.97 | loss  4.92 | ppl   137.17\n",
      "| epoch   5 | batch  2000/ 2250 | ms/batch  5.07 | loss  4.87 | ppl   130.10\n",
      "| epoch   5 | batch  2050/ 2250 | ms/batch  5.04 | loss  4.87 | ppl   129.99\n",
      "| epoch   5 | batch  2100/ 2250 | ms/batch  5.06 | loss  4.84 | ppl   126.18\n",
      "| epoch   5 | batch  2150/ 2250 | ms/batch  5.05 | loss  4.85 | ppl   127.72\n",
      "| epoch   5 | batch  2200/ 2250 | ms/batch  5.00 | loss  4.88 | ppl   131.97\n",
      "| epoch   5 | batch  2250/ 2250 | ms/batch  4.92 | loss  4.86 | ppl   129.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   5 | time: 14.15s | valid ppl   116.71\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   116.71\n",
      "| epoch   6 | batch    50/ 2250 | ms/batch  5.08 | loss  4.95 | ppl   141.68\n",
      "| epoch   6 | batch   100/ 2250 | ms/batch  5.02 | loss  4.79 | ppl   120.56\n",
      "| epoch   6 | batch   150/ 2250 | ms/batch  4.98 | loss  4.78 | ppl   119.46\n",
      "| epoch   6 | batch   200/ 2250 | ms/batch  4.97 | loss  4.78 | ppl   119.44\n",
      "| epoch   6 | batch   250/ 2250 | ms/batch  5.02 | loss  4.80 | ppl   121.27\n",
      "| epoch   6 | batch   300/ 2250 | ms/batch  4.98 | loss  4.87 | ppl   129.71\n",
      "| epoch   6 | batch   350/ 2250 | ms/batch  5.01 | loss  4.80 | ppl   121.07\n",
      "| epoch   6 | batch   400/ 2250 | ms/batch  5.59 | loss  4.82 | ppl   123.52\n",
      "| epoch   6 | batch   450/ 2250 | ms/batch  5.17 | loss  4.80 | ppl   121.89\n",
      "| epoch   6 | batch   500/ 2250 | ms/batch  4.89 | loss  4.79 | ppl   120.32\n",
      "| epoch   6 | batch   550/ 2250 | ms/batch  4.98 | loss  4.81 | ppl   122.38\n",
      "| epoch   6 | batch   600/ 2250 | ms/batch  4.99 | loss  4.81 | ppl   122.86\n",
      "| epoch   6 | batch   650/ 2250 | ms/batch  5.00 | loss  4.81 | ppl   122.36\n",
      "| epoch   6 | batch   700/ 2250 | ms/batch  4.99 | loss  4.81 | ppl   122.20\n",
      "| epoch   6 | batch   750/ 2250 | ms/batch  5.08 | loss  4.77 | ppl   118.46\n",
      "| epoch   6 | batch   800/ 2250 | ms/batch  5.15 | loss  4.85 | ppl   128.20\n",
      "| epoch   6 | batch   850/ 2250 | ms/batch  5.02 | loss  4.79 | ppl   120.69\n",
      "| epoch   6 | batch   900/ 2250 | ms/batch  5.02 | loss  4.84 | ppl   126.88\n",
      "| epoch   6 | batch   950/ 2250 | ms/batch  5.21 | loss  4.83 | ppl   125.81\n",
      "| epoch   6 | batch  1000/ 2250 | ms/batch  4.99 | loss  4.83 | ppl   125.75\n",
      "| epoch   6 | batch  1050/ 2250 | ms/batch  5.05 | loss  4.83 | ppl   125.00\n",
      "| epoch   6 | batch  1100/ 2250 | ms/batch  4.98 | loss  4.86 | ppl   129.21\n",
      "| epoch   6 | batch  1150/ 2250 | ms/batch  5.00 | loss  4.89 | ppl   132.45\n",
      "| epoch   6 | batch  1200/ 2250 | ms/batch  5.01 | loss  4.80 | ppl   121.64\n",
      "| epoch   6 | batch  1250/ 2250 | ms/batch  5.13 | loss  4.81 | ppl   122.89\n",
      "| epoch   6 | batch  1300/ 2250 | ms/batch  4.97 | loss  4.82 | ppl   124.22\n",
      "| epoch   6 | batch  1350/ 2250 | ms/batch  5.36 | loss  4.84 | ppl   125.90\n",
      "| epoch   6 | batch  1400/ 2250 | ms/batch  4.99 | loss  4.89 | ppl   132.74\n",
      "| epoch   6 | batch  1450/ 2250 | ms/batch  4.96 | loss  4.88 | ppl   131.81\n",
      "| epoch   6 | batch  1500/ 2250 | ms/batch  4.98 | loss  4.88 | ppl   132.12\n",
      "| epoch   6 | batch  1550/ 2250 | ms/batch  4.98 | loss  4.79 | ppl   120.88\n",
      "| epoch   6 | batch  1600/ 2250 | ms/batch  5.03 | loss  4.79 | ppl   120.77\n",
      "| epoch   6 | batch  1650/ 2250 | ms/batch  5.04 | loss  4.76 | ppl   116.69\n",
      "| epoch   6 | batch  1700/ 2250 | ms/batch  5.03 | loss  4.77 | ppl   118.50\n",
      "| epoch   6 | batch  1750/ 2250 | ms/batch  5.04 | loss  4.79 | ppl   119.76\n",
      "| epoch   6 | batch  1800/ 2250 | ms/batch  5.06 | loss  4.78 | ppl   119.34\n",
      "| epoch   6 | batch  1850/ 2250 | ms/batch  5.17 | loss  4.80 | ppl   120.95\n",
      "| epoch   6 | batch  1900/ 2250 | ms/batch  5.03 | loss  4.83 | ppl   125.57\n",
      "| epoch   6 | batch  1950/ 2250 | ms/batch  5.01 | loss  4.88 | ppl   131.68\n",
      "| epoch   6 | batch  2000/ 2250 | ms/batch  5.07 | loss  4.84 | ppl   125.85\n",
      "| epoch   6 | batch  2050/ 2250 | ms/batch  5.02 | loss  4.83 | ppl   125.60\n",
      "| epoch   6 | batch  2100/ 2250 | ms/batch  5.14 | loss  4.81 | ppl   122.21\n",
      "| epoch   6 | batch  2150/ 2250 | ms/batch  4.81 | loss  4.82 | ppl   123.61\n",
      "| epoch   6 | batch  2200/ 2250 | ms/batch  4.78 | loss  4.85 | ppl   127.37\n",
      "| epoch   6 | batch  2250/ 2250 | ms/batch  4.74 | loss  4.83 | ppl   124.82\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   6 | time: 14.28s | valid ppl   113.25\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   113.25\n",
      "| epoch   7 | batch    50/ 2250 | ms/batch  4.93 | loss  4.90 | ppl   134.53\n",
      "| epoch   7 | batch   100/ 2250 | ms/batch  4.76 | loss  4.76 | ppl   116.48\n",
      "| epoch   7 | batch   150/ 2250 | ms/batch  4.76 | loss  4.75 | ppl   115.03\n",
      "| epoch   7 | batch   200/ 2250 | ms/batch  4.76 | loss  4.74 | ppl   114.37\n",
      "| epoch   7 | batch   250/ 2250 | ms/batch  4.76 | loss  4.76 | ppl   116.88\n",
      "| epoch   7 | batch   300/ 2250 | ms/batch  4.74 | loss  4.84 | ppl   125.96\n",
      "| epoch   7 | batch   350/ 2250 | ms/batch  4.93 | loss  4.76 | ppl   116.27\n",
      "| epoch   7 | batch   400/ 2250 | ms/batch  4.76 | loss  4.79 | ppl   120.53\n",
      "| epoch   7 | batch   450/ 2250 | ms/batch  4.83 | loss  4.77 | ppl   118.36\n",
      "| epoch   7 | batch   500/ 2250 | ms/batch  4.80 | loss  4.76 | ppl   116.47\n",
      "| epoch   7 | batch   550/ 2250 | ms/batch  4.76 | loss  4.78 | ppl   119.34\n",
      "| epoch   7 | batch   600/ 2250 | ms/batch  4.77 | loss  4.77 | ppl   118.26\n",
      "| epoch   7 | batch   650/ 2250 | ms/batch  4.76 | loss  4.77 | ppl   117.97\n",
      "| epoch   7 | batch   700/ 2250 | ms/batch  4.75 | loss  4.77 | ppl   118.19\n",
      "| epoch   7 | batch   750/ 2250 | ms/batch  4.78 | loss  4.73 | ppl   113.79\n",
      "| epoch   7 | batch   800/ 2250 | ms/batch  4.75 | loss  4.82 | ppl   124.31\n",
      "| epoch   7 | batch   850/ 2250 | ms/batch  4.78 | loss  4.76 | ppl   116.73\n",
      "| epoch   7 | batch   900/ 2250 | ms/batch  4.87 | loss  4.81 | ppl   123.14\n",
      "| epoch   7 | batch   950/ 2250 | ms/batch  4.74 | loss  4.81 | ppl   122.46\n",
      "| epoch   7 | batch  1000/ 2250 | ms/batch  4.75 | loss  4.81 | ppl   122.38\n",
      "| epoch   7 | batch  1050/ 2250 | ms/batch  4.77 | loss  4.80 | ppl   121.85\n",
      "| epoch   7 | batch  1100/ 2250 | ms/batch  4.95 | loss  4.84 | ppl   125.92\n",
      "| epoch   7 | batch  1150/ 2250 | ms/batch  4.97 | loss  4.85 | ppl   127.71\n",
      "| epoch   7 | batch  1200/ 2250 | ms/batch  4.78 | loss  4.76 | ppl   116.75\n",
      "| epoch   7 | batch  1250/ 2250 | ms/batch  4.98 | loss  4.79 | ppl   120.35\n",
      "| epoch   7 | batch  1300/ 2250 | ms/batch  4.76 | loss  4.79 | ppl   120.67\n",
      "| epoch   7 | batch  1350/ 2250 | ms/batch  4.77 | loss  4.80 | ppl   121.63\n",
      "| epoch   7 | batch  1400/ 2250 | ms/batch  4.75 | loss  4.86 | ppl   129.03\n",
      "| epoch   7 | batch  1450/ 2250 | ms/batch  4.75 | loss  4.85 | ppl   128.37\n",
      "| epoch   7 | batch  1500/ 2250 | ms/batch  4.74 | loss  4.84 | ppl   126.83\n",
      "| epoch   7 | batch  1550/ 2250 | ms/batch  4.76 | loss  4.77 | ppl   117.77\n",
      "| epoch   7 | batch  1600/ 2250 | ms/batch  4.76 | loss  4.76 | ppl   116.49\n",
      "| epoch   7 | batch  1650/ 2250 | ms/batch  4.74 | loss  4.73 | ppl   113.75\n",
      "| epoch   7 | batch  1700/ 2250 | ms/batch  4.73 | loss  4.76 | ppl   116.23\n",
      "| epoch   7 | batch  1750/ 2250 | ms/batch  4.73 | loss  4.75 | ppl   115.38\n",
      "| epoch   7 | batch  1800/ 2250 | ms/batch  4.93 | loss  4.75 | ppl   115.33\n",
      "| epoch   7 | batch  1850/ 2250 | ms/batch  4.97 | loss  4.76 | ppl   117.21\n",
      "| epoch   7 | batch  1900/ 2250 | ms/batch  4.98 | loss  4.81 | ppl   123.22\n",
      "| epoch   7 | batch  1950/ 2250 | ms/batch  5.11 | loss  4.85 | ppl   128.35\n",
      "| epoch   7 | batch  2000/ 2250 | ms/batch  5.06 | loss  4.80 | ppl   122.10\n",
      "| epoch   7 | batch  2050/ 2250 | ms/batch  5.00 | loss  4.80 | ppl   122.07\n",
      "| epoch   7 | batch  2100/ 2250 | ms/batch  5.01 | loss  4.77 | ppl   118.50\n",
      "| epoch   7 | batch  2150/ 2250 | ms/batch  5.07 | loss  4.78 | ppl   119.20\n",
      "| epoch   7 | batch  2200/ 2250 | ms/batch  5.02 | loss  4.82 | ppl   123.70\n",
      "| epoch   7 | batch  2250/ 2250 | ms/batch  4.95 | loss  4.79 | ppl   120.24\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   7 | time: 13.87s | valid ppl   110.28\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   110.28\n",
      "| epoch   8 | batch    50/ 2250 | ms/batch  4.91 | loss  4.87 | ppl   130.47\n",
      "| epoch   8 | batch   100/ 2250 | ms/batch  4.74 | loss  4.73 | ppl   112.74\n",
      "| epoch   8 | batch   150/ 2250 | ms/batch  4.74 | loss  4.72 | ppl   111.74\n",
      "| epoch   8 | batch   200/ 2250 | ms/batch  4.75 | loss  4.73 | ppl   113.14\n",
      "| epoch   8 | batch   250/ 2250 | ms/batch  4.86 | loss  4.73 | ppl   113.05\n",
      "| epoch   8 | batch   300/ 2250 | ms/batch  5.00 | loss  4.81 | ppl   122.89\n",
      "| epoch   8 | batch   350/ 2250 | ms/batch  4.98 | loss  4.74 | ppl   113.87\n",
      "| epoch   8 | batch   400/ 2250 | ms/batch  4.98 | loss  4.75 | ppl   115.66\n",
      "| epoch   8 | batch   450/ 2250 | ms/batch  5.02 | loss  4.74 | ppl   114.81\n",
      "| epoch   8 | batch   500/ 2250 | ms/batch  5.03 | loss  4.73 | ppl   113.21\n",
      "| epoch   8 | batch   550/ 2250 | ms/batch  5.01 | loss  4.75 | ppl   115.17\n",
      "| epoch   8 | batch   600/ 2250 | ms/batch  5.03 | loss  4.76 | ppl   116.34\n",
      "| epoch   8 | batch   650/ 2250 | ms/batch  4.93 | loss  4.75 | ppl   115.22\n",
      "| epoch   8 | batch   700/ 2250 | ms/batch  4.87 | loss  4.75 | ppl   115.52\n",
      "| epoch   8 | batch   750/ 2250 | ms/batch  4.84 | loss  4.71 | ppl   111.06\n",
      "| epoch   8 | batch   800/ 2250 | ms/batch  4.86 | loss  4.81 | ppl   122.25\n",
      "| epoch   8 | batch   850/ 2250 | ms/batch  4.92 | loss  4.74 | ppl   114.45\n",
      "| epoch   8 | batch   900/ 2250 | ms/batch  4.82 | loss  4.79 | ppl   120.67\n",
      "| epoch   8 | batch   950/ 2250 | ms/batch  4.82 | loss  4.79 | ppl   120.43\n",
      "| epoch   8 | batch  1000/ 2250 | ms/batch  4.81 | loss  4.78 | ppl   118.75\n",
      "| epoch   8 | batch  1050/ 2250 | ms/batch  4.82 | loss  4.78 | ppl   119.12\n",
      "| epoch   8 | batch  1100/ 2250 | ms/batch  4.98 | loss  4.80 | ppl   121.82\n",
      "| epoch   8 | batch  1150/ 2250 | ms/batch  4.95 | loss  4.82 | ppl   124.43\n",
      "| epoch   8 | batch  1200/ 2250 | ms/batch  4.77 | loss  4.74 | ppl   114.06\n",
      "| epoch   8 | batch  1250/ 2250 | ms/batch  4.79 | loss  4.76 | ppl   116.44\n",
      "| epoch   8 | batch  1300/ 2250 | ms/batch  4.78 | loss  4.78 | ppl   118.73\n",
      "| epoch   8 | batch  1350/ 2250 | ms/batch  4.77 | loss  4.78 | ppl   118.81\n",
      "| epoch   8 | batch  1400/ 2250 | ms/batch  4.76 | loss  4.84 | ppl   126.16\n",
      "| epoch   8 | batch  1450/ 2250 | ms/batch  4.75 | loss  4.83 | ppl   125.63\n",
      "| epoch   8 | batch  1500/ 2250 | ms/batch  4.81 | loss  4.82 | ppl   124.09\n",
      "| epoch   8 | batch  1550/ 2250 | ms/batch  4.76 | loss  4.74 | ppl   113.89\n",
      "| epoch   8 | batch  1600/ 2250 | ms/batch  4.76 | loss  4.74 | ppl   114.25\n",
      "| epoch   8 | batch  1650/ 2250 | ms/batch  4.76 | loss  4.71 | ppl   110.86\n",
      "| epoch   8 | batch  1700/ 2250 | ms/batch  4.77 | loss  4.71 | ppl   111.04\n",
      "| epoch   8 | batch  1750/ 2250 | ms/batch  4.76 | loss  4.72 | ppl   112.71\n",
      "| epoch   8 | batch  1800/ 2250 | ms/batch  4.87 | loss  4.72 | ppl   112.35\n",
      "| epoch   8 | batch  1850/ 2250 | ms/batch  4.96 | loss  4.75 | ppl   115.41\n",
      "| epoch   8 | batch  1900/ 2250 | ms/batch  5.03 | loss  4.78 | ppl   119.22\n",
      "| epoch   8 | batch  1950/ 2250 | ms/batch  5.03 | loss  4.82 | ppl   124.51\n",
      "| epoch   8 | batch  2000/ 2250 | ms/batch  5.02 | loss  4.78 | ppl   119.07\n",
      "| epoch   8 | batch  2050/ 2250 | ms/batch  5.11 | loss  4.78 | ppl   118.69\n",
      "| epoch   8 | batch  2100/ 2250 | ms/batch  5.06 | loss  4.75 | ppl   115.32\n",
      "| epoch   8 | batch  2150/ 2250 | ms/batch  4.91 | loss  4.75 | ppl   115.52\n",
      "| epoch   8 | batch  2200/ 2250 | ms/batch  4.82 | loss  4.80 | ppl   121.01\n",
      "| epoch   8 | batch  2250/ 2250 | ms/batch  4.84 | loss  4.78 | ppl   118.79\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   8 | time: 13.96s | valid ppl   109.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   109.95\n",
      "| epoch   9 | batch    50/ 2250 | ms/batch  5.15 | loss  4.84 | ppl   126.38\n",
      "| epoch   9 | batch   100/ 2250 | ms/batch  4.98 | loss  4.70 | ppl   110.26\n",
      "| epoch   9 | batch   150/ 2250 | ms/batch  5.01 | loss  4.68 | ppl   108.31\n",
      "| epoch   9 | batch   200/ 2250 | ms/batch  4.90 | loss  4.70 | ppl   110.36\n",
      "| epoch   9 | batch   250/ 2250 | ms/batch  4.89 | loss  4.71 | ppl   110.54\n",
      "| epoch   9 | batch   300/ 2250 | ms/batch  4.87 | loss  4.79 | ppl   120.20\n",
      "| epoch   9 | batch   350/ 2250 | ms/batch  4.81 | loss  4.71 | ppl   110.94\n",
      "| epoch   9 | batch   400/ 2250 | ms/batch  4.76 | loss  4.73 | ppl   113.00\n",
      "| epoch   9 | batch   450/ 2250 | ms/batch  4.75 | loss  4.72 | ppl   112.52\n",
      "| epoch   9 | batch   500/ 2250 | ms/batch  4.77 | loss  4.71 | ppl   110.72\n",
      "| epoch   9 | batch   550/ 2250 | ms/batch  4.77 | loss  4.73 | ppl   112.89\n",
      "| epoch   9 | batch   600/ 2250 | ms/batch  4.91 | loss  4.73 | ppl   113.72\n",
      "| epoch   9 | batch   650/ 2250 | ms/batch  4.77 | loss  4.73 | ppl   112.94\n",
      "| epoch   9 | batch   700/ 2250 | ms/batch  4.80 | loss  4.75 | ppl   115.36\n",
      "| epoch   9 | batch   750/ 2250 | ms/batch  4.82 | loss  4.70 | ppl   109.61\n",
      "| epoch   9 | batch   800/ 2250 | ms/batch  4.83 | loss  4.77 | ppl   118.35\n",
      "| epoch   9 | batch   850/ 2250 | ms/batch  4.79 | loss  4.71 | ppl   111.40\n",
      "| epoch   9 | batch   900/ 2250 | ms/batch  4.77 | loss  4.76 | ppl   116.90\n",
      "| epoch   9 | batch   950/ 2250 | ms/batch  4.77 | loss  4.76 | ppl   116.22\n",
      "| epoch   9 | batch  1000/ 2250 | ms/batch  4.78 | loss  4.75 | ppl   115.98\n",
      "| epoch   9 | batch  1050/ 2250 | ms/batch  4.75 | loss  4.76 | ppl   116.43\n",
      "| epoch   9 | batch  1100/ 2250 | ms/batch  4.81 | loss  4.78 | ppl   119.69\n",
      "| epoch   9 | batch  1150/ 2250 | ms/batch  4.75 | loss  4.80 | ppl   121.97\n",
      "| epoch   9 | batch  1200/ 2250 | ms/batch  4.75 | loss  4.72 | ppl   112.35\n",
      "| epoch   9 | batch  1250/ 2250 | ms/batch  4.87 | loss  4.74 | ppl   113.88\n",
      "| epoch   9 | batch  1300/ 2250 | ms/batch  4.97 | loss  4.75 | ppl   115.15\n",
      "| epoch   9 | batch  1350/ 2250 | ms/batch  4.91 | loss  4.75 | ppl   115.92\n",
      "| epoch   9 | batch  1400/ 2250 | ms/batch  4.78 | loss  4.81 | ppl   122.20\n",
      "| epoch   9 | batch  1450/ 2250 | ms/batch  5.00 | loss  4.80 | ppl   121.90\n",
      "| epoch   9 | batch  1500/ 2250 | ms/batch  4.79 | loss  4.80 | ppl   121.36\n",
      "| epoch   9 | batch  1550/ 2250 | ms/batch  4.76 | loss  4.72 | ppl   111.95\n",
      "| epoch   9 | batch  1600/ 2250 | ms/batch  4.76 | loss  4.71 | ppl   110.77\n",
      "| epoch   9 | batch  1650/ 2250 | ms/batch  4.77 | loss  4.70 | ppl   109.53\n",
      "| epoch   9 | batch  1700/ 2250 | ms/batch  5.02 | loss  4.69 | ppl   109.39\n",
      "| epoch   9 | batch  1750/ 2250 | ms/batch  4.93 | loss  4.71 | ppl   110.98\n",
      "| epoch   9 | batch  1800/ 2250 | ms/batch  4.96 | loss  4.71 | ppl   111.18\n",
      "| epoch   9 | batch  1850/ 2250 | ms/batch  5.01 | loss  4.73 | ppl   113.30\n",
      "| epoch   9 | batch  1900/ 2250 | ms/batch  5.04 | loss  4.76 | ppl   116.90\n",
      "| epoch   9 | batch  1950/ 2250 | ms/batch  5.01 | loss  4.80 | ppl   121.79\n",
      "| epoch   9 | batch  2000/ 2250 | ms/batch  5.01 | loss  4.76 | ppl   116.24\n",
      "| epoch   9 | batch  2050/ 2250 | ms/batch  5.06 | loss  4.77 | ppl   117.39\n",
      "| epoch   9 | batch  2100/ 2250 | ms/batch  4.93 | loss  4.75 | ppl   115.17\n",
      "| epoch   9 | batch  2150/ 2250 | ms/batch  4.83 | loss  4.73 | ppl   112.96\n",
      "| epoch   9 | batch  2200/ 2250 | ms/batch  4.81 | loss  4.77 | ppl   118.32\n",
      "| epoch   9 | batch  2250/ 2250 | ms/batch  4.81 | loss  4.76 | ppl   116.35\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   9 | time: 13.91s | valid ppl   108.92\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   108.92\n",
      "| epoch  10 | batch    50/ 2250 | ms/batch  4.88 | loss  4.82 | ppl   124.19\n",
      "| epoch  10 | batch   100/ 2250 | ms/batch  4.77 | loss  4.68 | ppl   108.22\n",
      "| epoch  10 | batch   150/ 2250 | ms/batch  4.75 | loss  4.68 | ppl   107.77\n",
      "| epoch  10 | batch   200/ 2250 | ms/batch  4.76 | loss  4.68 | ppl   107.91\n",
      "| epoch  10 | batch   250/ 2250 | ms/batch  4.75 | loss  4.70 | ppl   110.13\n",
      "| epoch  10 | batch   300/ 2250 | ms/batch  4.78 | loss  4.77 | ppl   117.51\n",
      "| epoch  10 | batch   350/ 2250 | ms/batch  4.75 | loss  4.69 | ppl   109.14\n",
      "| epoch  10 | batch   400/ 2250 | ms/batch  4.74 | loss  4.71 | ppl   111.38\n",
      "| epoch  10 | batch   450/ 2250 | ms/batch  4.72 | loss  4.71 | ppl   110.58\n",
      "| epoch  10 | batch   500/ 2250 | ms/batch  4.76 | loss  4.69 | ppl   108.60\n",
      "| epoch  10 | batch   550/ 2250 | ms/batch  4.74 | loss  4.71 | ppl   111.13\n",
      "| epoch  10 | batch   600/ 2250 | ms/batch  4.77 | loss  4.72 | ppl   112.25\n",
      "| epoch  10 | batch   650/ 2250 | ms/batch  4.78 | loss  4.71 | ppl   110.66\n",
      "| epoch  10 | batch   700/ 2250 | ms/batch  4.78 | loss  4.72 | ppl   111.91\n",
      "| epoch  10 | batch   750/ 2250 | ms/batch  4.80 | loss  4.67 | ppl   106.61\n",
      "| epoch  10 | batch   800/ 2250 | ms/batch  4.76 | loss  4.76 | ppl   117.20\n",
      "| epoch  10 | batch   850/ 2250 | ms/batch  4.79 | loss  4.69 | ppl   109.33\n",
      "| epoch  10 | batch   900/ 2250 | ms/batch  4.96 | loss  4.74 | ppl   114.64\n",
      "| epoch  10 | batch   950/ 2250 | ms/batch  4.76 | loss  4.74 | ppl   114.05\n",
      "| epoch  10 | batch  1000/ 2250 | ms/batch  4.76 | loss  4.73 | ppl   113.34\n",
      "| epoch  10 | batch  1050/ 2250 | ms/batch  4.74 | loss  4.74 | ppl   114.50\n",
      "| epoch  10 | batch  1100/ 2250 | ms/batch  4.74 | loss  4.77 | ppl   117.43\n",
      "| epoch  10 | batch  1150/ 2250 | ms/batch  4.75 | loss  4.78 | ppl   119.67\n",
      "| epoch  10 | batch  1200/ 2250 | ms/batch  4.77 | loss  4.70 | ppl   109.83\n",
      "| epoch  10 | batch  1250/ 2250 | ms/batch  4.75 | loss  4.72 | ppl   112.27\n",
      "| epoch  10 | batch  1300/ 2250 | ms/batch  4.76 | loss  4.73 | ppl   113.49\n",
      "| epoch  10 | batch  1350/ 2250 | ms/batch  4.75 | loss  4.74 | ppl   114.17\n",
      "| epoch  10 | batch  1400/ 2250 | ms/batch  4.76 | loss  4.80 | ppl   121.86\n",
      "| epoch  10 | batch  1450/ 2250 | ms/batch  4.92 | loss  4.79 | ppl   120.30\n",
      "| epoch  10 | batch  1500/ 2250 | ms/batch  4.79 | loss  4.78 | ppl   119.66\n",
      "| epoch  10 | batch  1550/ 2250 | ms/batch  4.74 | loss  4.70 | ppl   110.04\n",
      "| epoch  10 | batch  1600/ 2250 | ms/batch  4.74 | loss  4.69 | ppl   108.65\n",
      "| epoch  10 | batch  1650/ 2250 | ms/batch  4.74 | loss  4.67 | ppl   106.35\n",
      "| epoch  10 | batch  1700/ 2250 | ms/batch  4.76 | loss  4.68 | ppl   107.63\n",
      "| epoch  10 | batch  1750/ 2250 | ms/batch  4.92 | loss  4.69 | ppl   108.69\n",
      "| epoch  10 | batch  1800/ 2250 | ms/batch  4.78 | loss  4.69 | ppl   109.08\n",
      "| epoch  10 | batch  1850/ 2250 | ms/batch  4.86 | loss  4.72 | ppl   112.07\n",
      "| epoch  10 | batch  1900/ 2250 | ms/batch  4.76 | loss  4.75 | ppl   115.74\n",
      "| epoch  10 | batch  1950/ 2250 | ms/batch  4.76 | loss  4.79 | ppl   120.22\n",
      "| epoch  10 | batch  2000/ 2250 | ms/batch  4.73 | loss  4.74 | ppl   114.79\n",
      "| epoch  10 | batch  2050/ 2250 | ms/batch  4.72 | loss  4.74 | ppl   114.21\n",
      "| epoch  10 | batch  2100/ 2250 | ms/batch  4.74 | loss  4.71 | ppl   111.41\n",
      "| epoch  10 | batch  2150/ 2250 | ms/batch  4.97 | loss  4.71 | ppl   111.17\n",
      "| epoch  10 | batch  2200/ 2250 | ms/batch  5.28 | loss  4.75 | ppl   115.51\n",
      "| epoch  10 | batch  2250/ 2250 | ms/batch  5.01 | loss  4.74 | ppl   113.88\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  10 | time: 13.74s | valid ppl   108.33\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   108.33\n",
      "| epoch  11 | batch    50/ 2250 | ms/batch  4.90 | loss  4.81 | ppl   122.79\n",
      "| epoch  11 | batch   100/ 2250 | ms/batch  4.76 | loss  4.67 | ppl   107.16\n",
      "| epoch  11 | batch   150/ 2250 | ms/batch  4.94 | loss  4.66 | ppl   105.86\n",
      "| epoch  11 | batch   200/ 2250 | ms/batch  4.74 | loss  4.66 | ppl   105.65\n",
      "| epoch  11 | batch   250/ 2250 | ms/batch  4.97 | loss  4.67 | ppl   106.36\n",
      "| epoch  11 | batch   300/ 2250 | ms/batch  5.24 | loss  4.77 | ppl   117.79\n",
      "| epoch  11 | batch   350/ 2250 | ms/batch  5.04 | loss  4.67 | ppl   106.67\n",
      "| epoch  11 | batch   400/ 2250 | ms/batch  4.98 | loss  4.70 | ppl   109.77\n",
      "| epoch  11 | batch   450/ 2250 | ms/batch  4.96 | loss  4.68 | ppl   108.25\n",
      "| epoch  11 | batch   500/ 2250 | ms/batch  4.97 | loss  4.67 | ppl   106.51\n",
      "| epoch  11 | batch   550/ 2250 | ms/batch  5.06 | loss  4.69 | ppl   109.21\n",
      "| epoch  11 | batch   600/ 2250 | ms/batch  5.03 | loss  4.70 | ppl   109.87\n",
      "| epoch  11 | batch   650/ 2250 | ms/batch  5.12 | loss  4.69 | ppl   109.28\n",
      "| epoch  11 | batch   700/ 2250 | ms/batch  5.12 | loss  4.70 | ppl   109.66\n",
      "| epoch  11 | batch   750/ 2250 | ms/batch  5.23 | loss  4.66 | ppl   105.27\n",
      "| epoch  11 | batch   800/ 2250 | ms/batch  4.97 | loss  4.75 | ppl   115.82\n",
      "| epoch  11 | batch   850/ 2250 | ms/batch  4.99 | loss  4.69 | ppl   108.69\n",
      "| epoch  11 | batch   900/ 2250 | ms/batch  5.07 | loss  4.73 | ppl   113.58\n",
      "| epoch  11 | batch   950/ 2250 | ms/batch  5.15 | loss  4.73 | ppl   113.25\n",
      "| epoch  11 | batch  1000/ 2250 | ms/batch  5.30 | loss  4.73 | ppl   112.80\n",
      "| epoch  11 | batch  1050/ 2250 | ms/batch  5.00 | loss  4.72 | ppl   111.91\n",
      "| epoch  11 | batch  1100/ 2250 | ms/batch  4.96 | loss  4.75 | ppl   115.27\n",
      "| epoch  11 | batch  1150/ 2250 | ms/batch  4.92 | loss  4.76 | ppl   116.89\n",
      "| epoch  11 | batch  1200/ 2250 | ms/batch  4.77 | loss  4.69 | ppl   108.75\n",
      "| epoch  11 | batch  1250/ 2250 | ms/batch  4.78 | loss  4.70 | ppl   109.86\n",
      "| epoch  11 | batch  1300/ 2250 | ms/batch  5.04 | loss  4.73 | ppl   112.74\n",
      "| epoch  11 | batch  1350/ 2250 | ms/batch  4.95 | loss  4.73 | ppl   113.12\n",
      "| epoch  11 | batch  1400/ 2250 | ms/batch  5.06 | loss  4.78 | ppl   119.50\n",
      "| epoch  11 | batch  1450/ 2250 | ms/batch  5.05 | loss  4.78 | ppl   118.93\n",
      "| epoch  11 | batch  1500/ 2250 | ms/batch  4.98 | loss  4.78 | ppl   119.09\n",
      "| epoch  11 | batch  1550/ 2250 | ms/batch  4.99 | loss  4.69 | ppl   108.93\n",
      "| epoch  11 | batch  1600/ 2250 | ms/batch  5.02 | loss  4.68 | ppl   107.28\n",
      "| epoch  11 | batch  1650/ 2250 | ms/batch  5.05 | loss  4.65 | ppl   104.95\n",
      "| epoch  11 | batch  1700/ 2250 | ms/batch  5.04 | loss  4.66 | ppl   105.81\n",
      "| epoch  11 | batch  1750/ 2250 | ms/batch  5.05 | loss  4.67 | ppl   106.99\n",
      "| epoch  11 | batch  1800/ 2250 | ms/batch  5.01 | loss  4.69 | ppl   108.68\n",
      "| epoch  11 | batch  1850/ 2250 | ms/batch  5.10 | loss  4.70 | ppl   109.93\n",
      "| epoch  11 | batch  1900/ 2250 | ms/batch  5.11 | loss  4.73 | ppl   113.06\n",
      "| epoch  11 | batch  1950/ 2250 | ms/batch  5.02 | loss  4.77 | ppl   118.02\n",
      "| epoch  11 | batch  2000/ 2250 | ms/batch  5.02 | loss  4.72 | ppl   111.69\n",
      "| epoch  11 | batch  2050/ 2250 | ms/batch  5.05 | loss  4.72 | ppl   111.88\n",
      "| epoch  11 | batch  2100/ 2250 | ms/batch  4.84 | loss  4.71 | ppl   110.56\n",
      "| epoch  11 | batch  2150/ 2250 | ms/batch  4.81 | loss  4.70 | ppl   110.17\n",
      "| epoch  11 | batch  2200/ 2250 | ms/batch  4.79 | loss  4.75 | ppl   115.24\n",
      "| epoch  11 | batch  2250/ 2250 | ms/batch  4.77 | loss  4.73 | ppl   113.10\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  11 | time: 14.20s | valid ppl   106.55\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   106.55\n",
      "| epoch  12 | batch    50/ 2250 | ms/batch  5.16 | loss  4.80 | ppl   121.27\n",
      "| epoch  12 | batch   100/ 2250 | ms/batch  4.95 | loss  4.66 | ppl   105.87\n",
      "| epoch  12 | batch   150/ 2250 | ms/batch  4.97 | loss  4.65 | ppl   104.11\n",
      "| epoch  12 | batch   200/ 2250 | ms/batch  4.98 | loss  4.64 | ppl   103.87\n",
      "| epoch  12 | batch   250/ 2250 | ms/batch  4.82 | loss  4.66 | ppl   105.67\n",
      "| epoch  12 | batch   300/ 2250 | ms/batch  4.81 | loss  4.74 | ppl   114.48\n",
      "| epoch  12 | batch   350/ 2250 | ms/batch  4.83 | loss  4.65 | ppl   104.88\n",
      "| epoch  12 | batch   400/ 2250 | ms/batch  4.88 | loss  4.69 | ppl   108.78\n",
      "| epoch  12 | batch   450/ 2250 | ms/batch  4.98 | loss  4.67 | ppl   106.87\n",
      "| epoch  12 | batch   500/ 2250 | ms/batch  5.02 | loss  4.65 | ppl   104.62\n",
      "| epoch  12 | batch   550/ 2250 | ms/batch  5.04 | loss  4.68 | ppl   107.35\n",
      "| epoch  12 | batch   600/ 2250 | ms/batch  5.00 | loss  4.68 | ppl   107.96\n",
      "| epoch  12 | batch   650/ 2250 | ms/batch  5.03 | loss  4.67 | ppl   106.99\n",
      "| epoch  12 | batch   700/ 2250 | ms/batch  5.10 | loss  4.69 | ppl   108.75\n",
      "| epoch  12 | batch   750/ 2250 | ms/batch  5.09 | loss  4.65 | ppl   104.35\n",
      "| epoch  12 | batch   800/ 2250 | ms/batch  5.03 | loss  4.73 | ppl   113.81\n",
      "| epoch  12 | batch   850/ 2250 | ms/batch  5.06 | loss  4.68 | ppl   107.71\n",
      "| epoch  12 | batch   900/ 2250 | ms/batch  5.04 | loss  4.72 | ppl   111.84\n",
      "| epoch  12 | batch   950/ 2250 | ms/batch  5.02 | loss  4.71 | ppl   110.98\n",
      "| epoch  12 | batch  1000/ 2250 | ms/batch  5.06 | loss  4.71 | ppl   111.21\n",
      "| epoch  12 | batch  1050/ 2250 | ms/batch  5.07 | loss  4.71 | ppl   111.30\n",
      "| epoch  12 | batch  1100/ 2250 | ms/batch  5.15 | loss  4.74 | ppl   114.10\n",
      "| epoch  12 | batch  1150/ 2250 | ms/batch  5.07 | loss  4.76 | ppl   116.29\n",
      "| epoch  12 | batch  1200/ 2250 | ms/batch  5.02 | loss  4.67 | ppl   106.84\n",
      "| epoch  12 | batch  1250/ 2250 | ms/batch  5.15 | loss  4.70 | ppl   109.87\n",
      "| epoch  12 | batch  1300/ 2250 | ms/batch  5.03 | loss  4.70 | ppl   110.26\n",
      "| epoch  12 | batch  1350/ 2250 | ms/batch  5.01 | loss  4.72 | ppl   112.00\n",
      "| epoch  12 | batch  1400/ 2250 | ms/batch  5.07 | loss  4.77 | ppl   118.01\n",
      "| epoch  12 | batch  1450/ 2250 | ms/batch  5.20 | loss  4.77 | ppl   117.58\n",
      "| epoch  12 | batch  1500/ 2250 | ms/batch  5.00 | loss  4.77 | ppl   118.21\n",
      "| epoch  12 | batch  1550/ 2250 | ms/batch  5.02 | loss  4.67 | ppl   107.18\n",
      "| epoch  12 | batch  1600/ 2250 | ms/batch  5.00 | loss  4.66 | ppl   106.04\n",
      "| epoch  12 | batch  1650/ 2250 | ms/batch  5.07 | loss  4.64 | ppl   103.30\n",
      "| epoch  12 | batch  1700/ 2250 | ms/batch  5.03 | loss  4.65 | ppl   104.57\n",
      "| epoch  12 | batch  1750/ 2250 | ms/batch  5.04 | loss  4.67 | ppl   106.94\n",
      "| epoch  12 | batch  1800/ 2250 | ms/batch  5.11 | loss  4.66 | ppl   106.07\n",
      "| epoch  12 | batch  1850/ 2250 | ms/batch  5.05 | loss  4.69 | ppl   108.51\n",
      "| epoch  12 | batch  1900/ 2250 | ms/batch  5.00 | loss  4.71 | ppl   111.24\n",
      "| epoch  12 | batch  1950/ 2250 | ms/batch  5.04 | loss  4.76 | ppl   116.81\n",
      "| epoch  12 | batch  2000/ 2250 | ms/batch  5.03 | loss  4.71 | ppl   111.33\n",
      "| epoch  12 | batch  2050/ 2250 | ms/batch  5.05 | loss  4.73 | ppl   112.74\n",
      "| epoch  12 | batch  2100/ 2250 | ms/batch  5.05 | loss  4.68 | ppl   108.28\n",
      "| epoch  12 | batch  2150/ 2250 | ms/batch  5.38 | loss  4.68 | ppl   107.96\n",
      "| epoch  12 | batch  2200/ 2250 | ms/batch  5.25 | loss  4.72 | ppl   112.34\n",
      "| epoch  12 | batch  2250/ 2250 | ms/batch  5.35 | loss  4.71 | ppl   111.56\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  12 | time: 14.33s | valid ppl   104.78\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   104.78\n",
      "| epoch  13 | batch    50/ 2250 | ms/batch  5.04 | loss  4.78 | ppl   119.68\n",
      "| epoch  13 | batch   100/ 2250 | ms/batch  4.73 | loss  4.65 | ppl   104.37\n",
      "| epoch  13 | batch   150/ 2250 | ms/batch  4.94 | loss  4.64 | ppl   103.34\n",
      "| epoch  13 | batch   200/ 2250 | ms/batch  4.94 | loss  4.63 | ppl   102.04\n",
      "| epoch  13 | batch   250/ 2250 | ms/batch  4.80 | loss  4.64 | ppl   103.75\n",
      "| epoch  13 | batch   300/ 2250 | ms/batch  4.78 | loss  4.73 | ppl   113.85\n",
      "| epoch  13 | batch   350/ 2250 | ms/batch  4.79 | loss  4.65 | ppl   104.21\n",
      "| epoch  13 | batch   400/ 2250 | ms/batch  4.85 | loss  4.67 | ppl   107.03\n",
      "| epoch  13 | batch   450/ 2250 | ms/batch  4.77 | loss  4.65 | ppl   104.67\n",
      "| epoch  13 | batch   500/ 2250 | ms/batch  4.76 | loss  4.64 | ppl   103.21\n",
      "| epoch  13 | batch   550/ 2250 | ms/batch  4.76 | loss  4.67 | ppl   106.94\n",
      "| epoch  13 | batch   600/ 2250 | ms/batch  4.81 | loss  4.68 | ppl   107.74\n",
      "| epoch  13 | batch   650/ 2250 | ms/batch  5.01 | loss  4.66 | ppl   106.01\n",
      "| epoch  13 | batch   700/ 2250 | ms/batch  4.94 | loss  4.67 | ppl   106.70\n",
      "| epoch  13 | batch   750/ 2250 | ms/batch  4.80 | loss  4.64 | ppl   103.27\n",
      "| epoch  13 | batch   800/ 2250 | ms/batch  4.79 | loss  4.73 | ppl   113.49\n",
      "| epoch  13 | batch   850/ 2250 | ms/batch  4.97 | loss  4.67 | ppl   106.51\n",
      "| epoch  13 | batch   900/ 2250 | ms/batch  4.79 | loss  4.71 | ppl   111.49\n",
      "| epoch  13 | batch   950/ 2250 | ms/batch  4.78 | loss  4.70 | ppl   109.75\n",
      "| epoch  13 | batch  1000/ 2250 | ms/batch  4.76 | loss  4.70 | ppl   109.84\n",
      "| epoch  13 | batch  1050/ 2250 | ms/batch  4.75 | loss  4.69 | ppl   109.37\n",
      "| epoch  13 | batch  1100/ 2250 | ms/batch  4.74 | loss  4.72 | ppl   112.71\n",
      "| epoch  13 | batch  1150/ 2250 | ms/batch  4.76 | loss  4.74 | ppl   114.73\n",
      "| epoch  13 | batch  1200/ 2250 | ms/batch  4.85 | loss  4.67 | ppl   106.45\n",
      "| epoch  13 | batch  1250/ 2250 | ms/batch  4.72 | loss  4.69 | ppl   108.75\n",
      "| epoch  13 | batch  1300/ 2250 | ms/batch  4.71 | loss  4.70 | ppl   110.15\n",
      "| epoch  13 | batch  1350/ 2250 | ms/batch  4.72 | loss  4.72 | ppl   112.39\n",
      "| epoch  13 | batch  1400/ 2250 | ms/batch  4.72 | loss  4.76 | ppl   116.95\n",
      "| epoch  13 | batch  1450/ 2250 | ms/batch  4.74 | loss  4.76 | ppl   117.09\n",
      "| epoch  13 | batch  1500/ 2250 | ms/batch  4.96 | loss  4.75 | ppl   115.19\n",
      "| epoch  13 | batch  1550/ 2250 | ms/batch  4.95 | loss  4.65 | ppl   105.03\n",
      "| epoch  13 | batch  1600/ 2250 | ms/batch  5.02 | loss  4.66 | ppl   105.48\n",
      "| epoch  13 | batch  1650/ 2250 | ms/batch  5.00 | loss  4.64 | ppl   103.19\n",
      "| epoch  13 | batch  1700/ 2250 | ms/batch  5.00 | loss  4.63 | ppl   103.02\n",
      "| epoch  13 | batch  1750/ 2250 | ms/batch  5.04 | loss  4.66 | ppl   105.20\n",
      "| epoch  13 | batch  1800/ 2250 | ms/batch  4.96 | loss  4.65 | ppl   104.78\n",
      "| epoch  13 | batch  1850/ 2250 | ms/batch  5.43 | loss  4.68 | ppl   108.17\n",
      "| epoch  13 | batch  1900/ 2250 | ms/batch  5.14 | loss  4.69 | ppl   109.20\n",
      "| epoch  13 | batch  1950/ 2250 | ms/batch  4.96 | loss  4.75 | ppl   115.74\n",
      "| epoch  13 | batch  2000/ 2250 | ms/batch  4.95 | loss  4.70 | ppl   110.16\n",
      "| epoch  13 | batch  2050/ 2250 | ms/batch  5.02 | loss  4.71 | ppl   110.84\n",
      "| epoch  13 | batch  2100/ 2250 | ms/batch  4.96 | loss  4.68 | ppl   107.24\n",
      "| epoch  13 | batch  2150/ 2250 | ms/batch  5.01 | loss  4.68 | ppl   107.36\n",
      "| epoch  13 | batch  2200/ 2250 | ms/batch  5.04 | loss  4.72 | ppl   112.01\n",
      "| epoch  13 | batch  2250/ 2250 | ms/batch  5.02 | loss  4.70 | ppl   109.67\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  13 | time: 13.99s | valid ppl   105.34\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  14 | batch    50/ 2250 | ms/batch  4.99 | loss  4.77 | ppl   118.34\n",
      "| epoch  14 | batch   100/ 2250 | ms/batch  4.77 | loss  4.63 | ppl   102.47\n",
      "| epoch  14 | batch   150/ 2250 | ms/batch  4.78 | loss  4.62 | ppl   101.57\n",
      "| epoch  14 | batch   200/ 2250 | ms/batch  4.75 | loss  4.64 | ppl   103.43\n",
      "| epoch  14 | batch   250/ 2250 | ms/batch  4.75 | loss  4.63 | ppl   102.44\n",
      "| epoch  14 | batch   300/ 2250 | ms/batch  4.76 | loss  4.72 | ppl   111.78\n",
      "| epoch  14 | batch   350/ 2250 | ms/batch  4.76 | loss  4.64 | ppl   103.60\n",
      "| epoch  14 | batch   400/ 2250 | ms/batch  4.77 | loss  4.66 | ppl   105.47\n",
      "| epoch  14 | batch   450/ 2250 | ms/batch  5.02 | loss  4.64 | ppl   103.64\n",
      "| epoch  14 | batch   500/ 2250 | ms/batch  4.99 | loss  4.62 | ppl   101.83\n",
      "| epoch  14 | batch   550/ 2250 | ms/batch  5.56 | loss  4.66 | ppl   105.63\n",
      "| epoch  14 | batch   600/ 2250 | ms/batch  5.05 | loss  4.66 | ppl   105.44\n",
      "| epoch  14 | batch   650/ 2250 | ms/batch  4.87 | loss  4.66 | ppl   105.80\n",
      "| epoch  14 | batch   700/ 2250 | ms/batch  5.02 | loss  4.66 | ppl   106.09\n",
      "| epoch  14 | batch   750/ 2250 | ms/batch  4.96 | loss  4.62 | ppl   101.65\n",
      "| epoch  14 | batch   800/ 2250 | ms/batch  5.00 | loss  4.72 | ppl   112.38\n",
      "| epoch  14 | batch   850/ 2250 | ms/batch  5.22 | loss  4.65 | ppl   104.42\n",
      "| epoch  14 | batch   900/ 2250 | ms/batch  4.97 | loss  4.70 | ppl   109.63\n",
      "| epoch  14 | batch   950/ 2250 | ms/batch  5.01 | loss  4.70 | ppl   109.58\n",
      "| epoch  14 | batch  1000/ 2250 | ms/batch  5.00 | loss  4.69 | ppl   108.79\n",
      "| epoch  14 | batch  1050/ 2250 | ms/batch  5.05 | loss  4.69 | ppl   108.61\n",
      "| epoch  14 | batch  1100/ 2250 | ms/batch  4.95 | loss  4.72 | ppl   112.58\n",
      "| epoch  14 | batch  1150/ 2250 | ms/batch  4.85 | loss  4.73 | ppl   113.57\n",
      "| epoch  14 | batch  1200/ 2250 | ms/batch  4.80 | loss  4.65 | ppl   104.98\n",
      "| epoch  14 | batch  1250/ 2250 | ms/batch  4.81 | loss  4.68 | ppl   107.94\n",
      "| epoch  14 | batch  1300/ 2250 | ms/batch  4.77 | loss  4.69 | ppl   108.56\n",
      "| epoch  14 | batch  1350/ 2250 | ms/batch  4.77 | loss  4.71 | ppl   110.64\n",
      "| epoch  14 | batch  1400/ 2250 | ms/batch  4.76 | loss  4.75 | ppl   115.06\n",
      "| epoch  14 | batch  1450/ 2250 | ms/batch  4.76 | loss  4.75 | ppl   115.37\n",
      "| epoch  14 | batch  1500/ 2250 | ms/batch  4.75 | loss  4.74 | ppl   114.83\n",
      "| epoch  14 | batch  1550/ 2250 | ms/batch  4.74 | loss  4.65 | ppl   104.58\n",
      "| epoch  14 | batch  1600/ 2250 | ms/batch  4.79 | loss  4.64 | ppl   103.70\n",
      "| epoch  14 | batch  1650/ 2250 | ms/batch  4.76 | loss  4.62 | ppl   101.55\n",
      "| epoch  14 | batch  1700/ 2250 | ms/batch  4.76 | loss  4.63 | ppl   102.17\n",
      "| epoch  14 | batch  1750/ 2250 | ms/batch  4.76 | loss  4.65 | ppl   104.38\n",
      "| epoch  14 | batch  1800/ 2250 | ms/batch  5.01 | loss  4.64 | ppl   103.99\n",
      "| epoch  14 | batch  1850/ 2250 | ms/batch  5.00 | loss  4.67 | ppl   106.92\n",
      "| epoch  14 | batch  1900/ 2250 | ms/batch  4.99 | loss  4.69 | ppl   109.19\n",
      "| epoch  14 | batch  1950/ 2250 | ms/batch  4.96 | loss  4.74 | ppl   114.79\n",
      "| epoch  14 | batch  2000/ 2250 | ms/batch  5.00 | loss  4.69 | ppl   109.21\n",
      "| epoch  14 | batch  2050/ 2250 | ms/batch  5.11 | loss  4.70 | ppl   109.40\n",
      "| epoch  14 | batch  2100/ 2250 | ms/batch  5.01 | loss  4.66 | ppl   105.18\n",
      "| epoch  14 | batch  2150/ 2250 | ms/batch  4.85 | loss  4.66 | ppl   105.58\n",
      "| epoch  14 | batch  2200/ 2250 | ms/batch  4.82 | loss  4.70 | ppl   110.48\n",
      "| epoch  14 | batch  2250/ 2250 | ms/batch  5.12 | loss  4.69 | ppl   109.32\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  14 | time: 14.04s | valid ppl   106.72\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  15 | batch    50/ 2250 | ms/batch  5.16 | loss  4.77 | ppl   117.91\n",
      "| epoch  15 | batch   100/ 2250 | ms/batch  5.05 | loss  4.62 | ppl   101.17\n",
      "| epoch  15 | batch   150/ 2250 | ms/batch  5.04 | loss  4.62 | ppl   101.03\n",
      "| epoch  15 | batch   200/ 2250 | ms/batch  5.07 | loss  4.63 | ppl   102.07\n",
      "| epoch  15 | batch   250/ 2250 | ms/batch  5.00 | loss  4.62 | ppl   101.47\n",
      "| epoch  15 | batch   300/ 2250 | ms/batch  5.08 | loss  4.69 | ppl   108.98\n",
      "| epoch  15 | batch   350/ 2250 | ms/batch  5.00 | loss  4.64 | ppl   103.20\n",
      "| epoch  15 | batch   400/ 2250 | ms/batch  5.07 | loss  4.65 | ppl   104.81\n",
      "| epoch  15 | batch   450/ 2250 | ms/batch  4.99 | loss  4.64 | ppl   103.39\n",
      "| epoch  15 | batch   500/ 2250 | ms/batch  5.00 | loss  4.62 | ppl   101.23\n",
      "| epoch  15 | batch   550/ 2250 | ms/batch  5.09 | loss  4.66 | ppl   105.34\n",
      "| epoch  15 | batch   600/ 2250 | ms/batch  5.03 | loss  4.65 | ppl   104.91\n",
      "| epoch  15 | batch   650/ 2250 | ms/batch  5.02 | loss  4.65 | ppl   104.32\n",
      "| epoch  15 | batch   700/ 2250 | ms/batch  5.06 | loss  4.65 | ppl   104.52\n",
      "| epoch  15 | batch   750/ 2250 | ms/batch  5.03 | loss  4.62 | ppl   101.60\n",
      "| epoch  15 | batch   800/ 2250 | ms/batch  5.06 | loss  4.71 | ppl   110.93\n",
      "| epoch  15 | batch   850/ 2250 | ms/batch  5.08 | loss  4.63 | ppl   102.96\n",
      "| epoch  15 | batch   900/ 2250 | ms/batch  5.06 | loss  4.69 | ppl   109.36\n",
      "| epoch  15 | batch   950/ 2250 | ms/batch  5.22 | loss  4.69 | ppl   108.39\n",
      "| epoch  15 | batch  1000/ 2250 | ms/batch  5.36 | loss  4.68 | ppl   108.16\n",
      "| epoch  15 | batch  1050/ 2250 | ms/batch  5.40 | loss  4.68 | ppl   107.44\n",
      "| epoch  15 | batch  1100/ 2250 | ms/batch  5.07 | loss  4.70 | ppl   109.88\n",
      "| epoch  15 | batch  1150/ 2250 | ms/batch  5.22 | loss  4.72 | ppl   112.12\n",
      "| epoch  15 | batch  1200/ 2250 | ms/batch  5.19 | loss  4.64 | ppl   103.25\n",
      "| epoch  15 | batch  1250/ 2250 | ms/batch  5.10 | loss  4.67 | ppl   106.84\n",
      "| epoch  15 | batch  1300/ 2250 | ms/batch  5.05 | loss  4.68 | ppl   108.17\n",
      "| epoch  15 | batch  1350/ 2250 | ms/batch  5.10 | loss  4.70 | ppl   109.53\n",
      "| epoch  15 | batch  1400/ 2250 | ms/batch  5.16 | loss  4.74 | ppl   114.25\n",
      "| epoch  15 | batch  1450/ 2250 | ms/batch  5.17 | loss  4.74 | ppl   114.45\n",
      "| epoch  15 | batch  1500/ 2250 | ms/batch  5.04 | loss  4.73 | ppl   113.46\n",
      "| epoch  15 | batch  1550/ 2250 | ms/batch  5.09 | loss  4.65 | ppl   104.19\n",
      "| epoch  15 | batch  1600/ 2250 | ms/batch  5.06 | loss  4.63 | ppl   102.86\n",
      "| epoch  15 | batch  1650/ 2250 | ms/batch  5.28 | loss  4.61 | ppl   100.15\n",
      "| epoch  15 | batch  1700/ 2250 | ms/batch  5.00 | loss  4.62 | ppl   101.14\n",
      "| epoch  15 | batch  1750/ 2250 | ms/batch  4.94 | loss  4.63 | ppl   102.71\n",
      "| epoch  15 | batch  1800/ 2250 | ms/batch  5.14 | loss  4.64 | ppl   103.63\n",
      "| epoch  15 | batch  1850/ 2250 | ms/batch  5.00 | loss  4.67 | ppl   106.34\n",
      "| epoch  15 | batch  1900/ 2250 | ms/batch  5.06 | loss  4.69 | ppl   108.89\n",
      "| epoch  15 | batch  1950/ 2250 | ms/batch  5.02 | loss  4.74 | ppl   114.30\n",
      "| epoch  15 | batch  2000/ 2250 | ms/batch  5.06 | loss  4.68 | ppl   107.59\n",
      "| epoch  15 | batch  2050/ 2250 | ms/batch  5.06 | loss  4.68 | ppl   108.22\n",
      "| epoch  15 | batch  2100/ 2250 | ms/batch  5.07 | loss  4.65 | ppl   104.27\n",
      "| epoch  15 | batch  2150/ 2250 | ms/batch  5.03 | loss  4.66 | ppl   105.37\n",
      "| epoch  15 | batch  2200/ 2250 | ms/batch  4.98 | loss  4.69 | ppl   109.19\n",
      "| epoch  15 | batch  2250/ 2250 | ms/batch  4.98 | loss  4.68 | ppl   108.31\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  15 | time: 14.42s | valid ppl   103.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   103.18\n",
      "| epoch  16 | batch    50/ 2250 | ms/batch  5.01 | loss  4.76 | ppl   116.62\n",
      "| epoch  16 | batch   100/ 2250 | ms/batch  4.97 | loss  4.62 | ppl   101.05\n",
      "| epoch  16 | batch   150/ 2250 | ms/batch  4.95 | loss  4.61 | ppl   100.68\n",
      "| epoch  16 | batch   200/ 2250 | ms/batch  4.95 | loss  4.62 | ppl   101.12\n",
      "| epoch  16 | batch   250/ 2250 | ms/batch  4.99 | loss  4.61 | ppl   100.50\n",
      "| epoch  16 | batch   300/ 2250 | ms/batch  5.07 | loss  4.70 | ppl   109.44\n",
      "| epoch  16 | batch   350/ 2250 | ms/batch  5.08 | loss  4.62 | ppl   101.03\n",
      "| epoch  16 | batch   400/ 2250 | ms/batch  4.98 | loss  4.65 | ppl   104.16\n",
      "| epoch  16 | batch   450/ 2250 | ms/batch  4.96 | loss  4.62 | ppl   101.63\n",
      "| epoch  16 | batch   500/ 2250 | ms/batch  5.00 | loss  4.61 | ppl   100.81\n",
      "| epoch  16 | batch   550/ 2250 | ms/batch  5.02 | loss  4.64 | ppl   103.60\n",
      "| epoch  16 | batch   600/ 2250 | ms/batch  5.04 | loss  4.65 | ppl   104.11\n",
      "| epoch  16 | batch   650/ 2250 | ms/batch  5.03 | loss  4.64 | ppl   103.47\n",
      "| epoch  16 | batch   700/ 2250 | ms/batch  5.06 | loss  4.65 | ppl   104.23\n",
      "| epoch  16 | batch   750/ 2250 | ms/batch  5.07 | loss  4.61 | ppl   100.61\n",
      "| epoch  16 | batch   800/ 2250 | ms/batch  5.06 | loss  4.69 | ppl   109.22\n",
      "| epoch  16 | batch   850/ 2250 | ms/batch  5.08 | loss  4.63 | ppl   102.27\n",
      "| epoch  16 | batch   900/ 2250 | ms/batch  5.18 | loss  4.68 | ppl   108.25\n",
      "| epoch  16 | batch   950/ 2250 | ms/batch  5.00 | loss  4.68 | ppl   108.10\n",
      "| epoch  16 | batch  1000/ 2250 | ms/batch  5.01 | loss  4.66 | ppl   106.16\n",
      "| epoch  16 | batch  1050/ 2250 | ms/batch  5.01 | loss  4.66 | ppl   105.58\n",
      "| epoch  16 | batch  1100/ 2250 | ms/batch  5.05 | loss  4.70 | ppl   109.72\n",
      "| epoch  16 | batch  1150/ 2250 | ms/batch  5.07 | loss  4.71 | ppl   111.46\n",
      "| epoch  16 | batch  1200/ 2250 | ms/batch  5.07 | loss  4.64 | ppl   103.84\n",
      "| epoch  16 | batch  1250/ 2250 | ms/batch  5.29 | loss  4.66 | ppl   105.52\n",
      "| epoch  16 | batch  1300/ 2250 | ms/batch  5.03 | loss  4.67 | ppl   106.84\n",
      "| epoch  16 | batch  1350/ 2250 | ms/batch  5.05 | loss  4.68 | ppl   107.57\n",
      "| epoch  16 | batch  1400/ 2250 | ms/batch  5.28 | loss  4.72 | ppl   112.58\n",
      "| epoch  16 | batch  1450/ 2250 | ms/batch  4.97 | loss  4.74 | ppl   114.77\n",
      "| epoch  16 | batch  1500/ 2250 | ms/batch  5.03 | loss  4.73 | ppl   113.14\n",
      "| epoch  16 | batch  1550/ 2250 | ms/batch  5.01 | loss  4.65 | ppl   104.37\n",
      "| epoch  16 | batch  1600/ 2250 | ms/batch  5.02 | loss  4.63 | ppl   102.05\n",
      "| epoch  16 | batch  1650/ 2250 | ms/batch  5.10 | loss  4.61 | ppl   100.35\n",
      "| epoch  16 | batch  1700/ 2250 | ms/batch  5.06 | loss  4.60 | ppl    99.78\n",
      "| epoch  16 | batch  1750/ 2250 | ms/batch  5.21 | loss  4.63 | ppl   102.54\n",
      "| epoch  16 | batch  1800/ 2250 | ms/batch  5.01 | loss  4.63 | ppl   102.73\n",
      "| epoch  16 | batch  1850/ 2250 | ms/batch  4.99 | loss  4.64 | ppl   103.93\n",
      "| epoch  16 | batch  1900/ 2250 | ms/batch  5.23 | loss  4.67 | ppl   107.13\n",
      "| epoch  16 | batch  1950/ 2250 | ms/batch  5.29 | loss  4.73 | ppl   113.51\n",
      "| epoch  16 | batch  2000/ 2250 | ms/batch  5.05 | loss  4.67 | ppl   106.98\n",
      "| epoch  16 | batch  2050/ 2250 | ms/batch  5.06 | loss  4.68 | ppl   107.35\n",
      "| epoch  16 | batch  2100/ 2250 | ms/batch  5.28 | loss  4.65 | ppl   104.51\n",
      "| epoch  16 | batch  2150/ 2250 | ms/batch  5.07 | loss  4.65 | ppl   104.14\n",
      "| epoch  16 | batch  2200/ 2250 | ms/batch  5.10 | loss  4.69 | ppl   108.84\n",
      "| epoch  16 | batch  2250/ 2250 | ms/batch  5.36 | loss  4.67 | ppl   106.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  16 | time: 14.38s | valid ppl   103.94\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  17 | batch    50/ 2250 | ms/batch  5.13 | loss  4.75 | ppl   115.83\n",
      "| epoch  17 | batch   100/ 2250 | ms/batch  4.96 | loss  4.60 | ppl    99.78\n",
      "| epoch  17 | batch   150/ 2250 | ms/batch  4.94 | loss  4.61 | ppl   100.46\n",
      "| epoch  17 | batch   200/ 2250 | ms/batch  5.09 | loss  4.60 | ppl    99.42\n",
      "| epoch  17 | batch   250/ 2250 | ms/batch  5.05 | loss  4.60 | ppl    99.06\n",
      "| epoch  17 | batch   300/ 2250 | ms/batch  4.99 | loss  4.68 | ppl   108.18\n",
      "| epoch  17 | batch   350/ 2250 | ms/batch  5.00 | loss  4.63 | ppl   102.32\n",
      "| epoch  17 | batch   400/ 2250 | ms/batch  4.98 | loss  4.64 | ppl   103.29\n",
      "| epoch  17 | batch   450/ 2250 | ms/batch  4.86 | loss  4.62 | ppl   101.11\n",
      "| epoch  17 | batch   500/ 2250 | ms/batch  4.82 | loss  4.60 | ppl    99.67\n",
      "| epoch  17 | batch   550/ 2250 | ms/batch  4.83 | loss  4.63 | ppl   102.74\n",
      "| epoch  17 | batch   600/ 2250 | ms/batch  4.79 | loss  4.64 | ppl   103.44\n",
      "| epoch  17 | batch   650/ 2250 | ms/batch  4.86 | loss  4.63 | ppl   102.48\n",
      "| epoch  17 | batch   700/ 2250 | ms/batch  4.77 | loss  4.63 | ppl   103.02\n",
      "| epoch  17 | batch   750/ 2250 | ms/batch  4.95 | loss  4.60 | ppl    99.56\n",
      "| epoch  17 | batch   800/ 2250 | ms/batch  4.79 | loss  4.70 | ppl   109.69\n",
      "| epoch  17 | batch   850/ 2250 | ms/batch  4.78 | loss  4.63 | ppl   102.49\n",
      "| epoch  17 | batch   900/ 2250 | ms/batch  4.77 | loss  4.67 | ppl   106.99\n",
      "| epoch  17 | batch   950/ 2250 | ms/batch  4.78 | loss  4.66 | ppl   105.73\n",
      "| epoch  17 | batch  1000/ 2250 | ms/batch  4.81 | loss  4.66 | ppl   105.68\n",
      "| epoch  17 | batch  1050/ 2250 | ms/batch  4.81 | loss  4.66 | ppl   105.78\n",
      "| epoch  17 | batch  1100/ 2250 | ms/batch  4.73 | loss  4.69 | ppl   109.21\n",
      "| epoch  17 | batch  1150/ 2250 | ms/batch  4.74 | loss  4.70 | ppl   110.17\n",
      "| epoch  17 | batch  1200/ 2250 | ms/batch  4.76 | loss  4.63 | ppl   102.71\n",
      "| epoch  17 | batch  1250/ 2250 | ms/batch  4.76 | loss  4.65 | ppl   104.33\n",
      "| epoch  17 | batch  1300/ 2250 | ms/batch  4.76 | loss  4.68 | ppl   107.87\n",
      "| epoch  17 | batch  1350/ 2250 | ms/batch  4.77 | loss  4.68 | ppl   107.93\n",
      "| epoch  17 | batch  1400/ 2250 | ms/batch  4.76 | loss  4.73 | ppl   112.86\n",
      "| epoch  17 | batch  1450/ 2250 | ms/batch  4.78 | loss  4.73 | ppl   113.03\n",
      "| epoch  17 | batch  1500/ 2250 | ms/batch  4.75 | loss  4.71 | ppl   111.48\n",
      "| epoch  17 | batch  1550/ 2250 | ms/batch  4.74 | loss  4.63 | ppl   102.85\n",
      "| epoch  17 | batch  1600/ 2250 | ms/batch  4.76 | loss  4.62 | ppl   101.80\n",
      "| epoch  17 | batch  1650/ 2250 | ms/batch  4.76 | loss  4.60 | ppl    99.14\n",
      "| epoch  17 | batch  1700/ 2250 | ms/batch  4.78 | loss  4.61 | ppl   100.09\n",
      "| epoch  17 | batch  1750/ 2250 | ms/batch  4.75 | loss  4.61 | ppl   100.28\n",
      "| epoch  17 | batch  1800/ 2250 | ms/batch  4.84 | loss  4.63 | ppl   102.35\n",
      "| epoch  17 | batch  1850/ 2250 | ms/batch  4.76 | loss  4.64 | ppl   103.44\n",
      "| epoch  17 | batch  1900/ 2250 | ms/batch  4.76 | loss  4.66 | ppl   105.93\n",
      "| epoch  17 | batch  1950/ 2250 | ms/batch  4.75 | loss  4.73 | ppl   112.76\n",
      "| epoch  17 | batch  2000/ 2250 | ms/batch  4.78 | loss  4.67 | ppl   106.38\n",
      "| epoch  17 | batch  2050/ 2250 | ms/batch  4.75 | loss  4.67 | ppl   106.34\n",
      "| epoch  17 | batch  2100/ 2250 | ms/batch  4.84 | loss  4.65 | ppl   104.87\n",
      "| epoch  17 | batch  2150/ 2250 | ms/batch  4.98 | loss  4.65 | ppl   104.77\n",
      "| epoch  17 | batch  2200/ 2250 | ms/batch  4.74 | loss  4.68 | ppl   107.98\n",
      "| epoch  17 | batch  2250/ 2250 | ms/batch  4.72 | loss  4.67 | ppl   106.45\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  17 | time: 13.82s | valid ppl   104.53\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  18 | batch    50/ 2250 | ms/batch  4.91 | loss  4.75 | ppl   115.81\n",
      "| epoch  18 | batch   100/ 2250 | ms/batch  4.89 | loss  4.59 | ppl    98.80\n",
      "| epoch  18 | batch   150/ 2250 | ms/batch  4.80 | loss  4.60 | ppl    99.20\n",
      "| epoch  18 | batch   200/ 2250 | ms/batch  4.77 | loss  4.59 | ppl    98.47\n",
      "| epoch  18 | batch   250/ 2250 | ms/batch  4.77 | loss  4.60 | ppl    99.32\n",
      "| epoch  18 | batch   300/ 2250 | ms/batch  4.78 | loss  4.68 | ppl   107.45\n",
      "| epoch  18 | batch   350/ 2250 | ms/batch  4.76 | loss  4.60 | ppl    99.28\n",
      "| epoch  18 | batch   400/ 2250 | ms/batch  4.76 | loss  4.63 | ppl   103.00\n",
      "| epoch  18 | batch   450/ 2250 | ms/batch  4.81 | loss  4.61 | ppl   100.59\n",
      "| epoch  18 | batch   500/ 2250 | ms/batch  4.99 | loss  4.60 | ppl    99.83\n",
      "| epoch  18 | batch   550/ 2250 | ms/batch  4.94 | loss  4.63 | ppl   102.46\n",
      "| epoch  18 | batch   600/ 2250 | ms/batch  4.84 | loss  4.64 | ppl   103.28\n",
      "| epoch  18 | batch   650/ 2250 | ms/batch  4.81 | loss  4.62 | ppl   101.04\n",
      "| epoch  18 | batch   700/ 2250 | ms/batch  4.82 | loss  4.62 | ppl   101.34\n",
      "| epoch  18 | batch   750/ 2250 | ms/batch  4.83 | loss  4.60 | ppl    99.05\n",
      "| epoch  18 | batch   800/ 2250 | ms/batch  4.76 | loss  4.69 | ppl   108.38\n",
      "| epoch  18 | batch   850/ 2250 | ms/batch  4.96 | loss  4.62 | ppl   101.51\n",
      "| epoch  18 | batch   900/ 2250 | ms/batch  4.86 | loss  4.67 | ppl   106.26\n",
      "| epoch  18 | batch   950/ 2250 | ms/batch  4.80 | loss  4.65 | ppl   105.05\n",
      "| epoch  18 | batch  1000/ 2250 | ms/batch  4.77 | loss  4.67 | ppl   106.17\n",
      "| epoch  18 | batch  1050/ 2250 | ms/batch  4.79 | loss  4.65 | ppl   104.62\n",
      "| epoch  18 | batch  1100/ 2250 | ms/batch  4.79 | loss  4.69 | ppl   108.42\n",
      "| epoch  18 | batch  1150/ 2250 | ms/batch  4.79 | loss  4.70 | ppl   109.63\n",
      "| epoch  18 | batch  1200/ 2250 | ms/batch  4.97 | loss  4.62 | ppl   101.51\n",
      "| epoch  18 | batch  1250/ 2250 | ms/batch  4.80 | loss  4.66 | ppl   106.16\n",
      "| epoch  18 | batch  1300/ 2250 | ms/batch  4.80 | loss  4.67 | ppl   106.30\n",
      "| epoch  18 | batch  1350/ 2250 | ms/batch  4.76 | loss  4.66 | ppl   105.62\n",
      "| epoch  18 | batch  1400/ 2250 | ms/batch  4.78 | loss  4.71 | ppl   111.50\n",
      "| epoch  18 | batch  1450/ 2250 | ms/batch  4.75 | loss  4.71 | ppl   111.38\n",
      "| epoch  18 | batch  1500/ 2250 | ms/batch  4.75 | loss  4.71 | ppl   111.35\n",
      "| epoch  18 | batch  1550/ 2250 | ms/batch  4.77 | loss  4.62 | ppl   101.55\n",
      "| epoch  18 | batch  1600/ 2250 | ms/batch  4.74 | loss  4.62 | ppl   101.17\n",
      "| epoch  18 | batch  1650/ 2250 | ms/batch  4.78 | loss  4.59 | ppl    98.27\n",
      "| epoch  18 | batch  1700/ 2250 | ms/batch  4.78 | loss  4.59 | ppl    98.88\n",
      "| epoch  18 | batch  1750/ 2250 | ms/batch  4.76 | loss  4.61 | ppl    99.99\n",
      "| epoch  18 | batch  1800/ 2250 | ms/batch  4.77 | loss  4.62 | ppl   101.40\n",
      "| epoch  18 | batch  1850/ 2250 | ms/batch  4.78 | loss  4.64 | ppl   103.14\n",
      "| epoch  18 | batch  1900/ 2250 | ms/batch  4.77 | loss  4.66 | ppl   106.01\n",
      "| epoch  18 | batch  1950/ 2250 | ms/batch  4.78 | loss  4.71 | ppl   111.48\n",
      "| epoch  18 | batch  2000/ 2250 | ms/batch  4.76 | loss  4.66 | ppl   105.86\n",
      "| epoch  18 | batch  2050/ 2250 | ms/batch  4.80 | loss  4.66 | ppl   105.41\n",
      "| epoch  18 | batch  2100/ 2250 | ms/batch  4.76 | loss  4.64 | ppl   103.94\n",
      "| epoch  18 | batch  2150/ 2250 | ms/batch  4.85 | loss  4.63 | ppl   102.62\n",
      "| epoch  18 | batch  2200/ 2250 | ms/batch  4.75 | loss  4.68 | ppl   107.66\n",
      "| epoch  18 | batch  2250/ 2250 | ms/batch  4.75 | loss  4.67 | ppl   106.81\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  18 | time: 13.75s | valid ppl   102.19\n",
      "-----------------------------------------------------------------------------------------\n",
      "New best model saved with perplexity:   102.19\n",
      "| epoch  19 | batch    50/ 2250 | ms/batch  4.90 | loss  4.74 | ppl   114.94\n",
      "| epoch  19 | batch   100/ 2250 | ms/batch  4.83 | loss  4.60 | ppl    99.35\n",
      "| epoch  19 | batch   150/ 2250 | ms/batch  4.90 | loss  4.59 | ppl    98.85\n",
      "| epoch  19 | batch   200/ 2250 | ms/batch  4.74 | loss  4.59 | ppl    98.28\n",
      "| epoch  19 | batch   250/ 2250 | ms/batch  4.75 | loss  4.60 | ppl    99.06\n",
      "| epoch  19 | batch   300/ 2250 | ms/batch  4.96 | loss  4.67 | ppl   106.43\n",
      "| epoch  19 | batch   350/ 2250 | ms/batch  4.77 | loss  4.61 | ppl   100.83\n",
      "| epoch  19 | batch   400/ 2250 | ms/batch  4.77 | loss  4.63 | ppl   102.43\n",
      "| epoch  19 | batch   450/ 2250 | ms/batch  5.08 | loss  4.60 | ppl    99.05\n",
      "| epoch  19 | batch   500/ 2250 | ms/batch  5.24 | loss  4.60 | ppl    99.07\n",
      "| epoch  19 | batch   550/ 2250 | ms/batch  4.97 | loss  4.62 | ppl   101.34\n",
      "| epoch  19 | batch   600/ 2250 | ms/batch  5.02 | loss  4.63 | ppl   102.49\n",
      "| epoch  19 | batch   650/ 2250 | ms/batch  5.12 | loss  4.61 | ppl   100.91\n",
      "| epoch  19 | batch   700/ 2250 | ms/batch  4.83 | loss  4.63 | ppl   102.37\n",
      "| epoch  19 | batch   750/ 2250 | ms/batch  5.00 | loss  4.60 | ppl    99.18\n",
      "| epoch  19 | batch   800/ 2250 | ms/batch  4.97 | loss  4.67 | ppl   106.45\n",
      "| epoch  19 | batch   850/ 2250 | ms/batch  4.82 | loss  4.60 | ppl    99.77\n",
      "| epoch  19 | batch   900/ 2250 | ms/batch  4.81 | loss  4.66 | ppl   105.62\n",
      "| epoch  19 | batch   950/ 2250 | ms/batch  4.79 | loss  4.65 | ppl   104.72\n",
      "| epoch  19 | batch  1000/ 2250 | ms/batch  4.78 | loss  4.65 | ppl   104.45\n",
      "| epoch  19 | batch  1050/ 2250 | ms/batch  4.75 | loss  4.64 | ppl   104.00\n",
      "| epoch  19 | batch  1100/ 2250 | ms/batch  4.76 | loss  4.68 | ppl   107.62\n",
      "| epoch  19 | batch  1150/ 2250 | ms/batch  4.76 | loss  4.69 | ppl   108.89\n",
      "| epoch  19 | batch  1200/ 2250 | ms/batch  4.87 | loss  4.61 | ppl   100.26\n",
      "| epoch  19 | batch  1250/ 2250 | ms/batch  4.95 | loss  4.64 | ppl   103.89\n",
      "| epoch  19 | batch  1300/ 2250 | ms/batch  4.82 | loss  4.66 | ppl   105.51\n",
      "| epoch  19 | batch  1350/ 2250 | ms/batch  4.77 | loss  4.66 | ppl   105.56\n",
      "| epoch  19 | batch  1400/ 2250 | ms/batch  4.74 | loss  4.71 | ppl   111.52\n",
      "| epoch  19 | batch  1450/ 2250 | ms/batch  4.74 | loss  4.72 | ppl   112.11\n",
      "| epoch  19 | batch  1500/ 2250 | ms/batch  4.82 | loss  4.72 | ppl   111.88\n",
      "| epoch  19 | batch  1550/ 2250 | ms/batch  5.14 | loss  4.62 | ppl   101.72\n",
      "| epoch  19 | batch  1600/ 2250 | ms/batch  4.99 | loss  4.60 | ppl    99.59\n",
      "| epoch  19 | batch  1650/ 2250 | ms/batch  5.01 | loss  4.59 | ppl    98.25\n",
      "| epoch  19 | batch  1700/ 2250 | ms/batch  4.96 | loss  4.60 | ppl    99.07\n",
      "| epoch  19 | batch  1750/ 2250 | ms/batch  5.04 | loss  4.60 | ppl    99.13\n",
      "| epoch  19 | batch  1800/ 2250 | ms/batch  4.99 | loss  4.61 | ppl   100.20\n",
      "| epoch  19 | batch  1850/ 2250 | ms/batch  4.98 | loss  4.63 | ppl   102.81\n",
      "| epoch  19 | batch  1900/ 2250 | ms/batch  5.14 | loss  4.65 | ppl   104.88\n",
      "| epoch  19 | batch  1950/ 2250 | ms/batch  5.05 | loss  4.71 | ppl   111.06\n",
      "| epoch  19 | batch  2000/ 2250 | ms/batch  5.00 | loss  4.65 | ppl   104.76\n",
      "| epoch  19 | batch  2050/ 2250 | ms/batch  5.00 | loss  4.65 | ppl   105.02\n",
      "| epoch  19 | batch  2100/ 2250 | ms/batch  5.02 | loss  4.64 | ppl   103.40\n",
      "| epoch  19 | batch  2150/ 2250 | ms/batch  5.03 | loss  4.63 | ppl   102.31\n",
      "| epoch  19 | batch  2200/ 2250 | ms/batch  5.06 | loss  4.67 | ppl   106.89\n",
      "| epoch  19 | batch  2250/ 2250 | ms/batch  5.09 | loss  4.65 | ppl   104.80\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  19 | time: 14.07s | valid ppl   103.03\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch  20 | batch    50/ 2250 | ms/batch  5.12 | loss  4.73 | ppl   113.28\n",
      "| epoch  20 | batch   100/ 2250 | ms/batch  4.98 | loss  4.59 | ppl    98.24\n",
      "| epoch  20 | batch   150/ 2250 | ms/batch  5.02 | loss  4.58 | ppl    97.95\n",
      "| epoch  20 | batch   200/ 2250 | ms/batch  4.92 | loss  4.59 | ppl    98.34\n",
      "| epoch  20 | batch   250/ 2250 | ms/batch  4.98 | loss  4.59 | ppl    98.31\n",
      "| epoch  20 | batch   300/ 2250 | ms/batch  4.84 | loss  4.66 | ppl   105.68\n",
      "| epoch  20 | batch   350/ 2250 | ms/batch  4.80 | loss  4.60 | ppl    99.12\n",
      "| epoch  20 | batch   400/ 2250 | ms/batch  4.90 | loss  4.62 | ppl   101.96\n",
      "| epoch  20 | batch   450/ 2250 | ms/batch  4.97 | loss  4.60 | ppl    99.82\n",
      "| epoch  20 | batch   500/ 2250 | ms/batch  4.95 | loss  4.59 | ppl    98.70\n",
      "| epoch  20 | batch   550/ 2250 | ms/batch  4.99 | loss  4.62 | ppl   101.91\n",
      "| epoch  20 | batch   600/ 2250 | ms/batch  5.04 | loss  4.62 | ppl   101.61\n",
      "| epoch  20 | batch   650/ 2250 | ms/batch  5.05 | loss  4.62 | ppl   101.18\n",
      "| epoch  20 | batch   700/ 2250 | ms/batch  5.22 | loss  4.62 | ppl   101.54\n",
      "| epoch  20 | batch   750/ 2250 | ms/batch  5.36 | loss  4.58 | ppl    97.48\n",
      "| epoch  20 | batch   800/ 2250 | ms/batch  5.25 | loss  4.67 | ppl   106.20\n",
      "| epoch  20 | batch   850/ 2250 | ms/batch  5.12 | loss  4.60 | ppl    99.57\n",
      "| epoch  20 | batch   900/ 2250 | ms/batch  5.09 | loss  4.66 | ppl   105.77\n",
      "| epoch  20 | batch   950/ 2250 | ms/batch  5.15 | loss  4.65 | ppl   104.88\n",
      "| epoch  20 | batch  1000/ 2250 | ms/batch  4.99 | loss  4.65 | ppl   104.16\n",
      "| epoch  20 | batch  1050/ 2250 | ms/batch  4.89 | loss  4.65 | ppl   104.34\n",
      "| epoch  20 | batch  1100/ 2250 | ms/batch  4.87 | loss  4.67 | ppl   106.55\n",
      "| epoch  20 | batch  1150/ 2250 | ms/batch  4.85 | loss  4.69 | ppl   108.34\n",
      "| epoch  20 | batch  1200/ 2250 | ms/batch  4.82 | loss  4.61 | ppl   100.46\n",
      "| epoch  20 | batch  1250/ 2250 | ms/batch  4.87 | loss  4.64 | ppl   103.55\n",
      "| epoch  20 | batch  1300/ 2250 | ms/batch  5.46 | loss  4.66 | ppl   105.55\n",
      "| epoch  20 | batch  1350/ 2250 | ms/batch  5.08 | loss  4.65 | ppl   104.78\n",
      "| epoch  20 | batch  1400/ 2250 | ms/batch  5.11 | loss  4.72 | ppl   111.74\n",
      "| epoch  20 | batch  1450/ 2250 | ms/batch  5.09 | loss  4.72 | ppl   111.92\n",
      "| epoch  20 | batch  1500/ 2250 | ms/batch  5.20 | loss  4.70 | ppl   109.50\n",
      "| epoch  20 | batch  1550/ 2250 | ms/batch  5.15 | loss  4.61 | ppl   100.90\n",
      "| epoch  20 | batch  1600/ 2250 | ms/batch  5.13 | loss  4.59 | ppl    98.31\n",
      "| epoch  20 | batch  1650/ 2250 | ms/batch  5.17 | loss  4.58 | ppl    97.26\n",
      "| epoch  20 | batch  1700/ 2250 | ms/batch  5.26 | loss  4.59 | ppl    98.33\n",
      "| epoch  20 | batch  1750/ 2250 | ms/batch  5.07 | loss  4.60 | ppl    99.08\n",
      "| epoch  20 | batch  1800/ 2250 | ms/batch  4.94 | loss  4.61 | ppl   100.59\n",
      "| epoch  20 | batch  1850/ 2250 | ms/batch  4.97 | loss  4.62 | ppl   101.82\n",
      "| epoch  20 | batch  1900/ 2250 | ms/batch  5.08 | loss  4.66 | ppl   105.57\n",
      "| epoch  20 | batch  1950/ 2250 | ms/batch  5.03 | loss  4.70 | ppl   110.18\n",
      "| epoch  20 | batch  2000/ 2250 | ms/batch  5.13 | loss  4.65 | ppl   104.33\n",
      "| epoch  20 | batch  2050/ 2250 | ms/batch  5.05 | loss  4.65 | ppl   104.55\n",
      "| epoch  20 | batch  2100/ 2250 | ms/batch  5.05 | loss  4.63 | ppl   102.15\n",
      "| epoch  20 | batch  2150/ 2250 | ms/batch  5.17 | loss  4.62 | ppl   101.39\n",
      "| epoch  20 | batch  2200/ 2250 | ms/batch  5.00 | loss  4.67 | ppl   106.81\n",
      "| epoch  20 | batch  2250/ 2250 | ms/batch  4.88 | loss  4.65 | ppl   104.95\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch  20 | time: 14.34s | valid ppl   103.76\n",
      "-----------------------------------------------------------------------------------------\n",
      "=========================================================================================\n",
      "| End of training | test ppl    93.63\n",
      "=========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "try:\n",
    "    best_val_ppl = float('inf')\n",
    "    print(\"Starting training...\")\n",
    "\n",
    "    val_ppls = []\n",
    "    train_ppls = []\n",
    "\n",
    "\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, train_data, criterion, optimizer, seq_length)\n",
    "        val_ppl = evaluate(model, valid_data, criterion, seq_length)\n",
    "        train_ppl = evaluate(model, train_data, criterion, seq_length)\n",
    "\n",
    "        val_ppls.append(val_ppl)\n",
    "        train_ppls.append(train_ppl)\n",
    "        \n",
    "        print('-' * 89)\n",
    "        print(f'| end of epoch {epoch:3d} | time: {(time.time() - epoch_start_time):5.2f}s | '\n",
    "              f'valid ppl {val_ppl:8.2f}')\n",
    "        print('-' * 89)\n",
    "        \n",
    "        # Save the model if validation performance improves\n",
    "        if val_ppl < best_val_ppl:\n",
    "            best_val_ppl = val_ppl\n",
    "            torch.save(model.state_dict(), 'best_rnn_model.pth')\n",
    "            print(f\"New best model saved with perplexity: {val_ppl:8.2f}\")\n",
    "\n",
    "    # Load best model and evaluate on test set\n",
    "    try:\n",
    "        model.load_state_dict(torch.load('best_rnn_model.pth'))\n",
    "        test_ppl = evaluate(model, test_data, criterion, seq_length)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test ppl {test_ppl:8.2f}')\n",
    "        print('=' * 89)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading best model: {e}\")\n",
    "        print(\"Evaluating with current model instead.\")\n",
    "        test_ppl = evaluate(model, test_data, criterion, seq_length)\n",
    "        print('=' * 89)\n",
    "        print(f'| End of training | test ppl {test_ppl:8.2f}')\n",
    "        print('=' * 89)\n",
    "\n",
    "except Exception as e:\n",
    "    import traceback\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text Sample:\n",
      "The game was considered a influenced potential to celebrate the night of all decade , and a hotel failures saw it came in <unk> . <unk> education the Union will break the Next Level \" , hair , her old of <unk> a , fourth up of <unk> one of the\n"
     ]
    }
   ],
   "source": [
    "# Function to generate text\n",
    "def generate_text(model, vocab, seed_text=\"the\", max_length=50):\n",
    "    \"\"\"Generate text using the trained model\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Create reverse vocab (index to token)\n",
    "    idx_to_token = {idx: token for token, idx in vocab.items()}\n",
    "    \n",
    "    # Convert seed text to tensor\n",
    "    if seed_text in vocab:\n",
    "        input_idx = vocab[seed_text]\n",
    "    else:\n",
    "        input_idx = vocab['<unk>']\n",
    "    \n",
    "    input_tensor = torch.tensor([[input_idx]], device=device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    \n",
    "    generated_tokens = [seed_text]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # Sample from the output distribution\n",
    "            probs = torch.softmax(output, dim=1)\n",
    "            next_token_idx = torch.multinomial(probs, 1).item()\n",
    "            \n",
    "            # Add generated token to output\n",
    "            generated_tokens.append(idx_to_token.get(next_token_idx, '<unk>'))\n",
    "            \n",
    "            # Update input for next iteration\n",
    "            input_tensor = torch.tensor([[next_token_idx]], device=device)\n",
    "    \n",
    "    return ' '.join(generated_tokens)\n",
    "\n",
    "# Generate and print some text\n",
    "print(\"\\nGenerated Text Sample:\")\n",
    "print(generate_text(model, vocab, seed_text=\"The\", max_length=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. provide a description (or illustration) of you architecture and discuss design choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is a simple recurrent neural network for language modeling. It consists of:\n",
    "\n",
    "1. **Embedding Layer (`nn.Embedding`)**  \n",
    "   - Converts token indices into dense vectors of size `embed_dim`.  \n",
    "\n",
    "2. **Recurrent Layer (`nn.RNN`)**  \n",
    "   - Processes embeddings sequentially to capture temporal dependencies.  \n",
    "\n",
    "3. **Dropout Layer (`nn.Dropout`)**  \n",
    "   - Regularizes the model to prevent overfitting.  \n",
    "\n",
    "4. **Fully Connected Layer (`nn.Linear`)**  \n",
    "   - Maps hidden states to vocabulary logits for next-word prediction.  \n",
    "\n",
    "## Design Choices\n",
    "- **Vanilla RNN**: Simple but may struggle with long-term dependencies.  \n",
    "- **Dropout**: Improves generalization.  \n",
    "- **Embedding Layer**: Captures word relationships efficiently.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. List hyper-parameters used by you model an discuss how you selected these values,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "\n",
    "```python\n",
    "embed_dim = 100        # Word embedding size\n",
    "hidden_dim = 256       # RNN hidden state size\n",
    "dropout_prob = 0.5     # Dropout rate for regularization\n",
    "num_epochs = 20        # Number of training iterations\n",
    "batch_size = 32        # Number of samples per batch\n",
    "seq_length = 29        # Number of unrolled time steps\n",
    "learning_rate = 0.001  # Step size for optimization\n",
    "vocab_size = 10000     # Reduced vocabulary size\n",
    "unk_threshold = 5      # Frequency threshold for unknown tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection Rationale\n",
    "\n",
    "- **`embed_dim = 100`**  \n",
    "  Balances computational efficiency and embedding quality.  \n",
    "\n",
    "- **`hidden_dim = 256`**  \n",
    "  Provides sufficient capacity without excessive overfitting.  \n",
    "\n",
    "- **`dropout_prob = 0.5`**  \n",
    "  Helps prevent overfitting by randomly dropping units.  \n",
    "\n",
    "- **`num_epochs = 20`**  \n",
    "  Predetermined by the assignment.  \n",
    "\n",
    "- **`batch_size = 32`**  \n",
    "  Standard size for stable and efficient training, tuned through trial and error.  \n",
    "\n",
    "- **`seq_length = 29`**  \n",
    "  Determines the number of unrolled time steps for training, optimized through trial and error.  \n",
    "\n",
    "- **`learning_rate = 0.001`**  \n",
    "  Common choice for stable Adam optimizer training.  \n",
    "\n",
    "- **`vocab_size = 10000`**  \n",
    "  Limits vocabulary to frequent words, reducing model complexity.  \n",
    "\n",
    "- **`unk_threshold = 5`**  \n",
    "  Filters rare words to improve generalization and efficiency.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c.Provide learning curves of perplexity vs. epoch on the training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "    # Plot learning curves\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Plot Validation and Train Perplexity\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(1, num_epochs + 1), val_ppls, label='Validation Perplexity')\n",
    "    plt.plot(range(1, num_epochs + 1), train_ppls, label='Train Perplexity')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Perplexity')\n",
    "    plt.title('Perplexity')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Provide final test set perplexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final test perplexity : 93.632\n"
     ]
    }
   ],
   "source": [
    "print(f\"Final test perplexity : {test_ppl:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
